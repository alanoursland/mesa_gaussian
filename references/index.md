This is a list of links to pdfs in the same folder. Each link contains the reference details.

[Sparse autoencoders find composed features in small toy models. (2024). LessWrong.](https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy)

[Toy Models of Superposition. (2022). Anthropic.](toy_models_of_superposition.pdf)

[What Causes Polysemanticity? (2024). ICLR Workshop.](what_causes_polysemanticity.pdf)

Arellano-Valle, R. B., Gómez, H. W., & Quintana, F. A. (2005). Statistical inference for a general class of asymmetric distributions. *Journal of Multivariate Analysis*, 92(1), 1-24.

[Arora, R., Basu, A., Mianjy, P., & Mukherjee, A. (2018). Understanding deep neural networks with rectified linear units. *International Conference on Learning Representations (ICLR)*.](understanding_deep_neural_networks_relu.pdf)

Badilini, F., Vaglio, M., Dubois, R., Roussel, P., Sarapa, N., Denjoy, I., Extramiana, F., & Maison-Blanche, P. (2008). Automatic analysis of cardiac repolarization morphology using Gaussian mesa function modeling. *Journal of Electrocardiology*, 41(6), 588-594.

[Balestriero, R., & Baraniuk, R. G. (2018). A Spline Theory of Deep Networks. *International Conference on Machine Learning (ICML)*.](mad_max_affine_spline.pdf)

[Bendale, A., & Boult, T. E. (2016). Towards Open Set Deep Networks. *CVPR*.](towards_open_set_deep_networks.pdf)

[Black, A., et al. (2022). Interpreting neural networks through the polytope lens. *AI Alignment Forum*.](interpreting_neural_networks_polytope_lens.pdf)

Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.

[Broomhead, D. S., & Lowe, D. (1988). Radial basis functions, multi-variable functional interpolation and adaptive networks. *Royal Signals and Radar Establishment Memorandum*, No. 4148.](radial_basis_functions.pdf)

[Cortes, C. & Vapnik, V. (1995). Support-vector networks. *Machine Learning*.](support_vector_networks.pdf)

De Maesschalck, R., Jouan-Rimbaud, D., & Massart, D. L. (2000). The Mahalanobis distance. *Chemometrics and Intelligent Laboratory Systems*, 50(1), 1–18.

Dubois, R., Maison-Blanche, P., Quenet, B., & Dreyfus, G. (2007). Automatic ECG wave extraction in long-term recordings using Gaussian mesa function models and nonlinear probability estimators. *Computer Methods and Programs in Biomedicine*.

[Erhan, D., Bengio, Y., Courville, A., & Vincent, P. (2009). Visualizing higher-layer features of a deep network. *Technical Report*, Université de Montréal.](visualizing_higher_layer_features.pdf)

Extramiana, F., Dubois, R., Vaglio, M., Roussel, P., Dreyfus, G., Badilini, F., Leenhardt, A., & Maison-Blanche, P. (2010). T-wave morphology parameters based on principal component analysis reproducibility and dependence on T-offset position. *Annals of Noninvasive Electrocardiology*, 15(1), 26–35.

[Fan, Z., Wang, H., & Xu, J. (2023). Deep ReLU networks have surprisingly simple polytopes. *arXiv preprint arXiv:2305.09145*.](deep_relu_networks_simple_polytopes.pdf)

[Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectifier neural networks. *Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)*, 315–323.](deep_sparse_rectifier.pdf)

[Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). Explaining and harnessing adversarial examples. *ICLR*.](explaining_harnessing_adversarial.pdf)

Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.

Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., & Stahel, W. A. (1986). *Robust Statistics: The Approach Based on Influence Functions*. Wiley.

[Hanin, B., & Rolnick, D. (2019). Deep ReLU networks have surprisingly few activation patterns. *NeurIPS*.](deep_relu_few_activation_patterns.pdf)

[Hartman, P. (1959). On functions representable as a difference of convex functions. Pacific Journal of Mathematics, 9(3), 707–713.](hartman_dc_functions.pdf)

[Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. *Neural Computation*, 14(8), 1771-1800.](contrastive_divergence.pdf)

Horn, R. A. & Johnson, C. R. (2012). *Matrix Analysis*. Cambridge University Press.

Huber, P. J. (1964). Robust estimation of a location parameter. *Annals of Mathematical Statistics*, 35(1), 73-101.

[Ioffe, S. & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. *ICML*.](batch_normalization.pdf)

Jolliffe, I. T. (2002). *Principal Component Analysis* (2nd ed.). Springer.

[LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. J. (2006). A tutorial on energy-based learning. In *Predicting Structured Data*. MIT Press.](energy_based_learning.pdf)

Ledoux, M. (2001). *The Concentration of Measure Phenomenon*. American Mathematical Society.

[Lee, S., Mammadov, E., & Ye, J. (2024). Defining neural network architecture through polytope structures of datasets. *Proceedings of the 41st International Conference on Machine Learning (ICML)*.](defining_nn_architecture_polytope.pdf)

[Li, Y., et al. (2025). The geometry of concepts: Sparse autoencoder feature structure. *Entropy*, 27(1), 42.](geometry_of_concepts_sae.pdf)

[Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., & Song, L. (2017). SphereFace: Deep hypersphere embedding for face recognition. *CVPR*.](sphereface.pdf)

Lovász, L. & Vempala, S. (2007). The geometry of logconcave distributions and sampling algorithms. *Random Structures & Algorithms*, 30(3), 307-358.

Mahalanobis, P. C. (1936). On the generalised distance in statistics. *Proceedings of the National Institute of Sciences of India*.

[McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. *The Bulletin of Mathematical Biophysics*, 5(4), 115–133.](mcculloch_pitts_1943.pdf)

McLachlan, G. J. (2019). Mahalanobis distance. *Wiley Interdisciplinary Reviews: Computational Statistics*, 11(4), e1462.

[Montúfar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. *NeurIPS*.](number_linear_regions.pdf)

[Moosavi-Dezfooli, S.-M., Fawzi, A., & Frossard, P. (2016). DeepFool: A simple and accurate method to fool deep neural networks. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2574–2582.](deepfool.pdf)

[Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. *ICML*.](relu_improve_rbm.pdf)

Nocedal, J. & Wright, S. J. (2006). *Numerical Optimization*. Springer.

[Olah, C., et al. (2020). Naturally occurring equivariance in neural networks. *Distill*.](https://distill.pub/2020/circuits/equivariance/)

[Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. *Nature*.](sparse_code_natural_images.pdf)

[Oursland, A. (2024a). Interpreting neural networks through Mahalanobis distance. *arXiv preprint arXiv:2410.19352*.](oursland_mahalanobis.pdf)

[Oursland, A. (2024b). Neural Networks Use Distance Metrics. *arXiv preprint arXiv:2411.17932v1*.](oursland_distance_metrics.pdf)

[Oursland, A. (2025). Neural Networks Learn Distance Metrics. *arXiv preprint arXiv:2502.02103*.](oursland_learn_distance_metrics.pdf)

[Park, K., et al. (2024). The Geometry of Categorical and Hierarchical Concepts in Large Language Models. *arXiv preprint arXiv:2406.01506*.](geometry_categorical_hierarchical.pdf)

[Pascanu, R., Montúfar, G. F., & Bengio, Y. (2014). On the number of response regions of deep feedforward networks with piecewise linear activations. *ICLR*.](response_regions_feedforward.pdf)

Reynolds, D. A. (2009). Gaussian mixture models. In *Encyclopedia of Biometrics* (pp. 659–663). Springer.

Rosasco, L., De Vito, E., Caponnetto, A., Piana, M., & Verri, A. (2004). Are loss functions all the same? *Neural Computation*, 16(5), 1063-1076.

Rosch, E. (1975). Cognitive representations of semantic categories. *Journal of Experimental Psychology: General*, 104(3), 192.

[Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. *Psychological Review*, 65(6), 386–408.](rosenblatt_perceptron.pdf)

[Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., Müller, E., & Kloft, M. (2018). Deep one-class classification. *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 4393–4402.](deep_one_class.pdf)

[Serra, T., Tjandraatmadja, C., & Ramalingam, S. (2018). Bounding and counting linear regions of deep neural networks. *ICML*.](bounding_counting_linear_regions.pdf)

[Snell, J., Swersky, K., & Zemel, R. (2017). Prototypical networks for few-shot learning. *Advances in Neural Information Processing Systems (NeurIPS)*, 30.](prototypical_networks.pdf)

[Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2014). Intriguing properties of neural networks. *International Conference on Learning Representations (ICLR)*.](intriguing_properties.pdf)

Tao, P. D., & An, L. H. (1997). Convex analysis approach to D.C. programming: Theory, algorithms and applications. *Acta Mathematica Vietnamica*, 22(1), 289–355.

Vapnik, V. N. (1995). *The Nature of Statistical Learning Theory*. Springer.

Vershynin, R. (2018). *High-Dimensional Probability*. Cambridge University Press.

[Weinberger, K. Q., & Saul, L. K. (2009). Distance metric learning for large margin nearest neighbor classification. *Journal of Machine Learning Research*, 10, 207–244.](lmnn_distance_metric.pdf)

Ziegler, G. M. (1995). *Lectures on Polytopes*. Springer.
