This is a list of links to pdfs in the same folder. Each link contains the reference details.

"Sparse autoencoders find composed features in small toy models." (2024). LessWrong.
"Toy Models of Superposition." (2022). Anthropic.
"What Causes Polysemanticity?" (2024). ICLR Workshop.
Arellano-Valle, R. B., Gómez, H. W., & Quintana, F. A. (2005). Statistical inference for a general class of asymmetric distributions. *Journal of Multivariate Analysis*, 92(1), 1-24.
Arora, R., Basu, A., Mianjy, P., & Mukherjee, A. (2018). Understanding deep neural networks with rectified linear units. *International Conference on Learning Representations (ICLR)*.
Badilini, F., Vaglio, M., Dubois, R., Roussel, P., Sarapa, N., Denjoy, I., Extramiana, F., & Maison-Blanche, P. (2008). Automatic analysis of cardiac repolarization morphology using Gaussian mesa function modeling. *Journal of Electrocardiology*, 41(6), 588-594.
Badilini, F., Vaglio, M., Dubois, R., Roussel, P., Sarapa, N., Denjoy, I., Extramiana, F., & Maison-Blanche, P. (2008). Automatic analysis of cardiac repolarization morphology using Gaussian mesa function modeling. *Journal of Electrocardiology*, 41(6), 588–594.
Balestriero, R., & Baraniuk, R. G. (2018). "Mad Max: Affine Spline Construction of Deep Neural Networks." *International Conference on Machine Learning (ICML)*.
Bendale, A., & Boult, T. E. (2016). "Towards Open Set Deep Networks." *CVPR*.
Black, A., et al. (2022). Interpreting neural networks through the polytope lens. *AI Alignment Forum*.
Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
Broomhead, D. S., & Lowe, D. (1988). Radial basis functions, multi-variable functional interpolation and adaptive networks. *Royal Signals and Radar Establishment Memorandum*, No. 4148.
Cortes, C. & Vapnik, V. (1995). Support-vector networks. *Machine Learning*.
De Maesschalck, R., Jouan-Rimbaud, D., & Massart, D. L. (2000). The Mahalanobis distance. *Chemometrics and Intelligent Laboratory Systems*, 50(1), 1–18.
Dubois, R., Maison-Blanche, P., Quenet, B., & Dreyfus, G. (2007). Automatic ECG wave extraction in long-term recordings using Gaussian mesa function models and nonlinear probability estimators. *Computer Methods and Programs in Biomedicine*.
Erhan, D., Bengio, Y., Courville, A., & Vincent, P. (2009). Visualizing higher-layer features of a deep network. *Technical Report*, Université de Montréal.
Extramiana, F., Dubois, R., Vaglio, M., Roussel, P., Dreyfus, G., Badilini, F., Leenhardt, A., & Maison-Blanche, P. (2010). T-wave morphology parameters based on principal component analysis reproducibility and dependence on T-offset position. *Annals of Noninvasive Electrocardiology*, 15(1), 26–35.
Fan, Y., et al. (2023). Deep ReLU networks have surprisingly simple polytopes. *arXiv:2305.09145*.
Fan, Z., Wang, H., & Xu, J. (2023). Deep ReLU networks have surprisingly simple polytopes. *arXiv preprint arXiv:2305.09145*.
Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectifier neural networks. *Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)*, 315–323.
Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). Explaining and harnessing adversarial examples. *ICLR*.
Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., & Stahel, W. A. (1986). *Robust Statistics: The Approach Based on Influence Functions*. Wiley.
Hanin, B., & Rolnick, D. (2019). Deep ReLU networks have surprisingly few activation patterns. *NeurIPS*.
Hartman, P. (1959). On functions representable as a difference of convex functions. Pacific Journal of Mathematics, 9(3), 707–713.
Hinton, G. E. (2002). "Training products of experts by minimizing contrastive divergence." *Neural Computation*, 14(8), 1771-1800.
Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. *Neural Computation*.
Horn, R. A. & Johnson, C. R. (2012). *Matrix Analysis*. Cambridge University Press.
Huber, P. J. (1964). Robust estimation of a location parameter. *Annals of Mathematical Statistics*, 35(1), 73-101.
Ioffe, S. & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. *ICML*.
Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. *Proceedings of the 32nd International Conference on Machine Learning (ICML)*, 448–456.
Jolliffe, I. T. (2002). *Principal Component Analysis* (2nd ed.). Springer.
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. J. (2006). A tutorial on energy-based learning. In *Predicting Structured Data*. MIT Press.
Ledoux, M. (2001). *The Concentration of Measure Phenomenon*. American Mathematical Society.
Lee, S., Mammadov, E., & Ye, J. (2024). Defining neural network architecture through polytope structures of datasets. *Proceedings of the 41st International Conference on Machine Learning (ICML)*.
Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., & Wattenberg, M. (2025). The geometry of concepts: Sparse autoencoder feature structure. *Entropy*, 27(1), 42.
Li, X., Tegmark, M., et al. (2025). The geometry of concepts: Sparse autoencoder feature structure. *Entropy*, arXiv:2410.19750.
Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., & Song, L. (2017). SphereFace: Deep hypersphere embedding for face recognition. *CVPR*.
Lovász, L. & Vempala, S. (2007). The geometry of logconcave distributions and sampling algorithms. *Random Structures & Algorithms*, 30(3), 307-358.
Mahalanobis, P. C. (1936). "On the generalised distance in statistics." *Proceedings of the National Institute of Sciences of India*.
McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. *The Bulletin of Mathematical Biophysics*, 5(4), 115–133.
McLachlan, G. J. (2019). Mahalanobis distance. *Wiley Interdisciplinary Reviews: Computational Statistics*, 11(4), e1462.
Montúfar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. *NeurIPS*.
Moosavi-Dezfooli, S. M., et al. (2016). "DeepFool: a simple and accurate method to fool deep neural networks." *CVPR*.
Moosavi-Dezfooli, S.-M., Fawzi, A., & Frossard, P. (2016). DeepFool: A simple and accurate method to fool deep neural networks. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2574–2582.
Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. *ICML*.
Nocedal, J. & Wright, S. J. (2006). *Numerical Optimization*. Springer.
Olah, C., et al. (2020). Naturally occurring equivariance in neural networks. *Distill*.
Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. *Nature*.
Oursland, A. (2024a). Interpreting neural networks through Mahalanobis distance. *arXiv preprint arXiv:2410.19352*.
Oursland, A. (2024b). "Neural Networks Use Distance Metrics." *arXiv preprint arXiv:2411.17932v1*. 
Oursland, A. (2025). "Neural Networks Learn Distance Metrics." *arXiv preprint arXiv:2502.02103*.
Park, K., et al. (2024). "The Geometry of Categorical and Hierarchical Concepts in Large Language Models." *arXiv preprint arXiv:2406.01506*.
Pascanu, R., Montúfar, G. F., & Bengio, Y. (2014). On the number of response regions of deep feedforward networks with piecewise linear activations. *ICLR*.
Reynolds, D. A. (2009). Gaussian mixture models. In *Encyclopedia of Biometrics* (pp. 659–663). Springer.
Rosasco, L., De Vito, E., Caponnetto, A., Piana, M., & Verri, A. (2004). Are loss functions all the same? *Neural Computation*, 16(5), 1063-1076.
Rosch, E. (1975). "Cognitive representations of semantic categories." *Journal of Experimental Psychology: General*, 104(3), 192.
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. *Psychological Review*, 65(6), 386–408.
Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., Müller, E., & Kloft, M. (2018). Deep one-class classification. *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 4393–4402.
Serra, T., Tjandraatmadja, C., & Ramalingam, S. (2018). Bounding and counting linear regions of deep neural networks. *ICML*.
Snell, J., Swersky, K., & Zemel, R. (2017). Prototypical networks for few-shot learning. *Advances in Neural Information Processing Systems (NeurIPS)*, 30.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2014). Intriguing properties of neural networks. *International Conference on Learning Representations (ICLR)*.
Tao, P. D., & An, L. H. (1997). Convex analysis approach to D.C. programming: Theory, algorithms and applications. *Acta Mathematica Vietnamica*, 22(1), 289–355.
This is a list of links to pdfs in the same folder. Each link contains the reference details.
Vapnik, V. N. (1995). *The Nature of Statistical Learning Theory*. Springer.
Vershynin, R. (2018). *High-Dimensional Probability*. Cambridge University Press.
Weinberger, K. Q., & Saul, L. K. (2009). Distance metric learning for large margin nearest neighbor classification. *Journal of Machine Learning Research*, 10, 207–244.
Ziegler, G. M. (1995). *Lectures on Polytopes*. Springer.
