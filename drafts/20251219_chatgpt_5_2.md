# Draft Outline: Polyhedral Mesa Gaussians and ReLU Networks

## Abstract

We introduce **Polyhedral Mesa Gaussians (PMGs)**, a theoretical generalization of Gaussian Mesa Functions that extends flat-topped, asymmetric Gaussian models from one dimension to multivariate and polyhedral settings. PMGs are defined using one-sided, variance-weighted distances from half-space constraints, yielding convex plateau regions surrounded by Gaussian-like decay without requiring piecewise definitions. We show that these structures arise naturally from the algebra of standard linear layers with ReLU activations: a ReLU computes a one-sided distance to a hyperplane, and compositions of such operations define polyhedral prototype regions. This leads to an interpretive inversion of the usual view of neural activations—activation magnitude reflects **deviation from a prototype region**, while zero activation corresponds to membership in an extended region of maximal compatibility rather than failure of detection [Oursland, 2024a]. We situate this perspective within existing theory and empirical work showing that ReLU networks partition representation space into simple polytopes, and we review evidence consistent with bounded, constraint-based representations. Finally, we identify a concrete open empirical prediction of the framework: the emergence of paired upper-lower constraints along latent dimensions, corresponding to two-sided bounds or antipodal hyperplane normals. The paper is explicitly **interpretive and theoretical** in nature; it proposes no new network architectures or training procedures, but aims to clarify the geometric and probabilistic structure already implicit in standard ReLU networks.

---

## 1. Introduction

### 1.1 Motivation: What is a ReLU activation actually measuring?

Rectified Linear Unit (ReLU) networks are most often described using the language of *feature detection*. In this standard interpretation, each neuron is treated as a template matcher: the weight vector represents a preferred feature, the dot product measures similarity between the input and that feature, and the ReLU nonlinearity suppresses negative evidence while allowing positive evidence to accumulate. Large activations are therefore taken to indicate strong presence or high confidence that a particular feature has been detected [Nair & Hinton, 2010; Goodfellow et al., 2016; Oursland, 2024b].

While intuitive, this interpretation relies on geometric assumptions that are not enforced by standard ReLU architectures [Oursland, 2024b]. Similarity-based reasoning is typically grounded in cosine similarity, which requires explicit normalization of both the input and the weight vectors. By contrast, the computation performed by a ReLU neuron is an unnormalized affine function,
[
h = \mathrm{ReLU}(w^\top x + b),
]
whose magnitude depends jointly on angular alignment, input norm, weight norm, and bias. Without normalization, the raw dot product does not measure similarity: an input that is poorly aligned with (w) can produce a larger activation than a well-aligned input simply by having larger magnitude. As a result, interpreting ReLU activations as similarity or confidence imports structure that the model itself does not guarantee [Ioffe & Szegedy, 2015; Liu et al., 2017].

An alternative interpretation follows directly from the literal geometry of the affine transformation. The equation (w^\top x + b = 0) defines a hyperplane in input space, and the quantity ((w^\top x + b)/|w|) corresponds to the signed distance from that hyperplane. Applying a ReLU clips this signed distance to one side, yielding a *one-sided distance*: the neuron outputs zero for all inputs lying on one side of the hyperplane and increases linearly with distance on the other. Under this view, a ReLU neuron does not measure how well an input matches a template; it measures how far the input deviates from an implicit constraint.

This distance-based interpretation suggests a fundamentally different way of understanding ReLU networks. Collections of ReLU neurons define systems of half-space constraints, and regions of low or zero activation correspond to intersections of these constraints. Rather than detecting point-like prototypes, ReLU networks carve input space into regions bounded by learned hyperplanes. Activations then encode deviation from these regions, naturally leading to geometric and probabilistic interpretations in terms of distance, boundaries, and constraint satisfaction [Oursland, 2024b; Oursland, 2025; MontA?far et al., 2014; Balestriero & Baraniuk, 2018].


### 1.2 Contributions

This work makes the following contributions:

1. **Mesa Mahalanobis Distance.**
   We formalize a *Mesa Mahalanobis Distance* (MMD), a unified closed-form construction that replaces the piecewise definition of Gaussian Mesa Functions (GMFs) with a rectified-distance formulation. This recasts GMFs as exponentials of variance-weighted, one-sided deviations, clarifying their structure and enabling straightforward generalization beyond the univariate case [Dubois et al., 2007; Badilini et al., 2008].

2. **Multivariate and Polyhedral Mesa Gaussians.**
   We generalize the univariate construction to multivariate settings by decomposing Mahalanobis distance into rectified component distances, yielding multivariate mesa regions with hyperrectangular plateaus. We further relax orthogonality assumptions to define *Polyhedral Mesa Gaussians*, in which plateau regions are arbitrary convex polytopes formed by intersections of half-space constraints. This connects mesa constructions to hinge-loss penalties, product-of-experts models, and energy-based formulations [Cortes & Vapnik, 1995; Hinton, 2002; LeCun et al., 2006].

3. **Interpretation of ReLU Networks.**
   We provide a geometric and probabilistic interpretation of standard ReLU networks as implicitly computing and composing polyhedral mesa structures. In this view, ReLU activations represent one-sided distances from learned hyperplanes, layers correspond to successive compositions of constraint systems, and zero-activation regions define extended prototype regions rather than point-like templates [Montúfar et al., 2014; Balestriero & Baraniuk, 2018].

4. **Empirical Landscape and Open Predictions.**
   We review empirical and theoretical literature demonstrating that trained ReLU networks organize representations polyhedrally, from piecewise-linear partitions and simple learned polytopes to concept geometry in large language models. We synthesize this evidence with the mesa framework and identify a concrete, currently untested empirical signature: whether networks learn *two-sided bounds*—paired constraints that jointly delimit acceptable ranges along latent feature dimensions—corresponding to box-like mesa geometry [Fan et al., 2023; Park et al., 2024].

### 1.3 Scope and non-claims

This paper is deliberately limited in scope. Its goal is to clarify the geometric and probabilistic structure *implicit* in standard ReLU networks, not to propose a new modeling paradigm or to advance performance claims. To avoid misinterpretation, we explicitly state several non-claims.

First, **we do not propose a new training method or architecture**. All constructions and interpretations in this work apply to standard feedforward networks with linear layers and ReLU activations trained using conventional objectives and optimization procedures. The framework is intended to be descriptive and interpretive, not prescriptive [Goodfellow et al., 2016].

Second, **we do not claim that all networks learn box-shaped (hyperrectangular) regions or antipodal constraint pairs**. While box-like mesas arise naturally in the multivariate Gaussian generalization, they represent a special case of the more general polyhedral construction. Empirical evidence suggests that learned concept regions are often simplices or other minimal polytopes rather than symmetric boxes, particularly under implicit regularization and sparsity pressures [Fan et al., 2023; Park et al., 2024]. Antipodal constraint pairs are therefore treated as a specific, testable hypothesis—relevant when networks learn two-sided bounds along latent dimensions—but not as a universal prediction.

Third, **we do not claim that ReLU activations are literal probabilities**. Although exponential envelopes of mesa distances yield well-defined probability densities in the Polyhedral Mesa Gaussian construction, the raw activations computed by a network layer are best interpreted as *energies* or *deviation measures*, not normalized likelihoods. This places the framework closer to energy-based modeling than to explicit generative density estimation, where normalization constants are often intractable or irrelevant for discrimination [LeCun et al., 2006; Hinton, 2002].

Finally, **we do not claim empirical completeness**. While we review a substantial body of theoretical and empirical literature supporting polytope-based representations in ReLU networks, the paper intentionally identifies open questions—particularly regarding two-sided bounds and local whitening structure—rather than asserting that they are already resolved. The intent is to articulate a coherent geometric framework and to make explicit which empirical signatures would support or refute it.

By clearly separating what is claimed from what is not, this work aims to provide a stable conceptual foundation that complements existing analyses of ReLU networks without overextending its conclusions.

---

## 2. Background: Gaussian Mesa Functions (Prior Art)

### 2.1 Gaussian Mesa Functions in Signal Processing

Gaussian Mesa Functions (GMFs) were introduced in the biomedical signal processing literature as a practical solution to a specific modeling problem: how to accurately capture the morphology of electrocardiogram (ECG) waveforms, which often exhibit sustained maxima rather than sharply peaked profiles. Standard Gaussian functions, while mathematically convenient, impose a single curvature scale and strict symmetry, making them poorly suited to represent signals with flat tops, asymmetric rise and fall times, or prolonged activation phases [Dubois et al., 2007].

The defining characteristic of a GMF is its **mesa-shaped geometry**: a central plateau region of constant amplitude flanked by decaying Gaussian tails. This geometry allows the peak height, plateau width, and left/right decay rates to be controlled independently. In the ECG setting, these parameters admit direct physiological interpretations (e.g., wave duration, asymmetry, and amplitude), which motivated their adoption for tasks such as wave delineation, morphology tracking, and repolarization analysis [Badilini et al., 2008; Extramiana et al., 2010].

For the purposes of this work, the biological application is incidental. What matters is the geometric insight embodied by GMFs: **useful signal representations sometimes require flat-topped, bounded regions rather than point-centered peaks**. The mesa geometry explicitly separates three notions that are conflated in a standard Gaussian—location, curvature, and extent—and demonstrates that a plateau of maximal response can be essential rather than pathological. In this sense, GMFs serve as an existence proof that flat-top geometries arise naturally when modeling structured data, even in domains traditionally dominated by smooth, unimodal distributions.

Despite their practical success, classical GMFs have important limitations. They are defined *piecewise*, with separate expressions for the left tail, plateau, and right tail. This makes them algebraically cumbersome, awkward to optimize, and difficult to generalize beyond one-dimensional time series. Moreover, the original formulation is intrinsically univariate: extending GMFs to higher-dimensional spaces or to arbitrary orientations requires ad hoc constructions and loses the clean interpretability that made the original model attractive [Dubois et al., 2007].

These limitations motivate the reformulation pursued in this paper. By recasting GMFs in terms of rectified, variance-weighted distances, the mesa geometry can be expressed in a unified closed form and naturally extended to multivariate and polyhedral settings. The goal is not to revisit ECG analysis, but to extract and generalize the underlying geometric idea—**a bounded region of maximal response defined by distances to constraints**—and to show how this idea connects directly to the algebra of ReLU networks.

### 2.2 Why Gaussian Mesa Functions Matter Beyond Biology

Gaussian Mesa Functions matter beyond their original biomedical context because they provide a concrete **existence proof** for a class of representations that is largely absent from mainstream statistical modeling: **flat-topped, bounded, asymmetric analogues of Gaussians**.

In much of statistics and machine learning, the Gaussian distribution plays a privileged role. It is mathematically convenient, analytically tractable, and often justified by asymptotic arguments such as the Central Limit Theorem. However, these justifications implicitly assume that the quantity of interest has a *single optimal point* and that deviations from that point are smoothly and symmetrically penalized. GMFs demonstrate that this assumption can be wrong in practice, even in domains where Gaussians are otherwise successful [Dubois et al., 2007].

The key contribution of GMFs is not biological realism but **geometric realism**. They encode the idea that there may exist a *range* of values that are equally acceptable, optimal, or typical, and that penalties should only accrue once this range is exceeded. Within the mesa, variation is irrelevant; outside it, deviation matters. This structure cannot be captured by any single-peaked, strictly convex distribution without either distorting the tails or sacrificing interpretability.

Importantly, GMFs also allow **asymmetric penalties**: the cost of deviating above the acceptable range can differ from the cost of deviating below it. In ECG analysis, this asymmetry corresponds to different physiological dynamics on the rising and falling edges of a waveform, but the abstraction is far more general. Many real-world systems tolerate deviations differently depending on direction—for example, undersupply versus oversupply, latency versus throughput, or under- versus over-regularization—yet are routinely modeled with symmetric losses for convenience [Huber, 1964; Hampel et al., 1986].

From this perspective, GMFs should be understood as members of a broader family of **robust, bounded-response models**, closely related to ideas in robust statistics and margin-based learning. The flat-top region plays a role analogous to an ε-insensitive zone in support vector regression, where errors within a margin are ignored and only boundary violations contribute to the loss [Vapnik, 1995]. What distinguishes GMFs is that they embed this margin directly into a probabilistic form, rather than treating it as an external loss heuristic.

Crucially for the present work, GMFs show that **plateau geometries are not pathological edge cases**. They arise naturally when the task demands tolerance, invariance, or insensitivity over a region rather than precision around a point. This observation motivates asking whether similar geometries might appear implicitly in modern machine learning systems—not because they were designed to, but because the algebra they implement makes such structures natural.

By reframing GMFs as geometric objects rather than domain-specific signal models, this paper treats them as a prototype for a more general representational principle: **probability mass concentrated on regions rather than points, bounded by learned constraints rather than centered on learned templates**. This reframing opens a path from a niche signal-processing tool to a general interpretation of ReLU-based representations.

---

## 3. Mesa Mahalanobis Distance (Univariate)

### 3.1 ReLU Decomposition of Absolute Value

We begin from the standard univariate Mahalanobis distance,
[
D_M(x;\mu,\sigma) ;=; \frac{|x-\mu|}{\sigma},
]
which appears in the exponent of a one-dimensional Gaussian density and measures variance-weighted deviation from a center (\mu) [Mahalanobis, 1936]. The key observation is that the absolute value admits an exact decomposition into two rectified linear components. Using (\mathrm{ReLU}(z)=\max(0,z)), we have the identity
[
|z| ;=; \mathrm{ReLU}(z) + \mathrm{ReLU}(-z).
]
Setting (z=x-\mu) yields
[
|x-\mu| ;=; \mathrm{ReLU}(x-\mu) + \mathrm{ReLU}(\mu-x).
]
This algebraic decomposition is elementary, but it is structurally consequential: it expresses the symmetric deviation (|x-\mu|) as the sum of two *one-sided* deviations.

Geometrically, each ReLU term corresponds to a half-space constraint relative to the threshold (\mu). The term (\mathrm{ReLU}(x-\mu)) is zero whenever (x\le \mu) and increases linearly for (x>\mu); it is therefore a one-sided distance that activates only when the input lies to the "right" of the boundary. Likewise, (\mathrm{ReLU}(\mu-x)) is zero whenever (x\ge \mu) and increases linearly for (x<\mu), activating only on the "left" side. In one dimension, these correspond to distances to the boundary point (\mu) measured from opposite half-lines; in higher dimensions, the same construction generalizes to distances to opposite half-spaces defined by hyperplanes.

This perspective recasts absolute deviation as a *pair* of half-space distances whose supports are disjoint: at most one term is nonzero at any (x\neq \mu). That disjoint-support property is the mechanism that later enables mesa constructions: by shifting the activation thresholds of these two one-sided distances outward, one can create an interval around (\mu) in which both terms vanish, producing a flat plateau. In the next section we use this idea to define a "mesa" variant of Mahalanobis distance that yields Gaussian-like tails with a region of zero deviation at the center.

### 3.2 Introducing the Mesa Gap

The ReLU decomposition in Section 3.1 expresses absolute deviation from (\mu) as the sum of two one-sided deviations. To obtain a *mesa*—a region of zero deviation over an interval rather than at a single point—we introduce a **gap** by shifting the activation thresholds of these one-sided terms outward.

Let (\delta \ge 0) denote a half-width parameter. Instead of penalizing any departure from (\mu), we ignore deviations that remain within the interval ([\mu-\delta,\mu+\delta]). This is implemented by replacing the two ReLU terms
(\mathrm{ReLU}(x-\mu)) and (\mathrm{ReLU}(\mu-x))
with their shifted counterparts:
[
\mathrm{ReLU}(x-\mu-\delta), \qquad \mathrm{ReLU}(\mu-x-\delta).
]
These shifts create three regimes:

* **Right tail** ((x > \mu+\delta)): the first term activates and grows linearly as (x) exceeds (\mu+\delta).
* **Plateau** ((\mu-\delta \le x \le \mu+\delta)): both terms are zero, producing a flat region of zero deviation.
* **Left tail** ((x < \mu-\delta)): the second term activates and grows linearly as (x) falls below (\mu-\delta).

We define the resulting variance-weighted deviation as the **Mesa Mahalanobis Distance**,
[
D_{MM}(x;\mu,\sigma,\delta)
;=;
\frac{ \mathrm{ReLU}(x-\mu-\delta) + \mathrm{ReLU}(\mu-x-\delta)}{\sigma}.
]
Equivalently, writing it by cases makes the geometry explicit:
[
D_{MM}(x;\mu,\sigma,\delta)

\begin{cases}
\frac{x-\mu-\delta}{\sigma}, & x>\mu+\delta,[0.5em]
0, & \mu-\delta \le x \le \mu+\delta,[0.5em]
\frac{\mu-\delta-x}{\sigma}, & x<\mu-\delta.
\end{cases}
]
The parameter (\delta) therefore controls the width of the zero-deviation region, and (\sigma) controls the tail scale outside the plateau, exactly as in the standard Mahalanobis distance but with an added insensitivity zone.

This construction is closely related in form to margin-based losses in statistical learning. In particular, the use of a rectified deviation beyond a margin parallels (\varepsilon)-insensitive penalties in support vector regression: deviations within the margin incur zero cost, while deviations beyond it incur a linear penalty (or quadratic penalty under squaring) [Vapnik, 1995]. Here the same mechanism is used not as a loss function per se, but as a building block for a mesa-shaped Gaussian envelope in the next section.

### 3.3 Gaussian Mesa Function (Closed Form)

Classical Gaussian Mesa Functions (GMFs) were introduced as *piecewise* constructions: a constant plateau joined to Gaussian tails on either side [Dubois et al., 2007]. The Mesa Mahalanobis Distance provides a unified way to recover the same behavior by placing the mesa structure entirely inside the distance term and then applying the usual Gaussian envelope.

Define the **Gaussian Mesa Function** as
[
G_{\text{mesa}}(x)
;=;
A \exp!\left(-\tfrac{1}{2} D_{MM}(x;\mu,\sigma,\delta)^2\right),
]
where (A>0) is an amplitude parameter and (D_{MM}) is the mesa distance from Section 3.2. This expression is a direct analogue of a standard Gaussian written in Mahalanobis form, with the sole change being the replacement (D_M \mapsto D_{MM}) [Mahalanobis, 1936].

Although this definition is *not* piecewise, it induces exactly the desired mesa geometry:

* **Inside the plateau** ((\mu-\delta \le x \le \mu+\delta)), we have (D_{MM}(x)=0), hence
  [
  G_{\text{mesa}}(x)=A,
  ]
  producing a flat top of constant height.

* **Outside the plateau**, the distance grows linearly with "excess deviation" beyond the boundary. For example, for (x>\mu+\delta),
  [
  D_{MM}(x)=\frac{x-\mu-\delta}{\sigma}
  \quad\Longrightarrow\quad
  G_{\text{mesa}}(x)
  = A\exp!\left(-\frac{(x-\mu-\delta)^2}{2\sigma^2}\right),
  ]
  i.e., a Gaussian tail whose effective center is shifted to the boundary point (\mu+\delta). The left tail is analogous with boundary (\mu-\delta).

Thus, the mesa function can be written as a single closed form in terms of rectified deviations, while still matching the behavior of the classical GMF: constant plateau and Gaussian decay outside. In particular, the plateau width (2\delta) decouples the *extent* of maximal response from the *curvature* of the tails, resolving the primary limitation of ordinary Gaussians in mesa-like regimes [Dubois et al., 2007; Badilini et al., 2008].

This formulation emphasizes two points that will be central in later sections:

1. **Unified expression.** The mesa geometry is encoded entirely in the distance (D_{MM}), so the resulting mesa Gaussian has the same exponential-of-squared-distance structure as an ordinary Gaussian.

2. **No explicit case distinction is required.** The piecewise behavior emerges automatically from the ReLU gating in (D_{MM}). This avoids the "clunky" piecewise definition used in the original GMF literature while preserving the exact plateau-and-tails geometry.

In the next sections we generalize this construction by (i) allowing asymmetric tail scales and (ii) lifting the rectified distance idea to multivariate and polyhedral constraint systems, where plateau regions become hyperrectangles or general polytopes.

### 3.4 Remarks

We close the univariate construction with several remarks that clarify its analytical properties and its broader significance.

#### Differentiability

The Mesa Mahalanobis Distance (D_{MM}(x)) is continuous everywhere and piecewise linear in (x). Its derivative is zero throughout the interior of the plateau and constant (with opposite signs) in the two tails. Nondifferentiability occurs only at the boundary points (x=\mu\pm\delta), where the active ReLU switches. Consequently, the Gaussian Mesa Function
[
G_{\text{mesa}}(x)=A\exp!\left(-\tfrac{1}{2}D_{MM}(x)^2\right)
]
is continuous and once differentiable ((C^1)) everywhere, but not twice differentiable at the plateau boundaries. This regularity matches that of standard ReLU-based models and is sufficient for gradient-based optimization methods used throughout modern machine learning [Nair & Hinton, 2010].

#### Relationship to hinge loss

The mesa construction is closely related to margin-based loss functions. In particular, the one-sided term (\mathrm{ReLU}(x-\mu-\delta)) is algebraically identical to a hinge loss with margin (\delta), up to a sign convention. Classical hinge loss penalizes points *inside* a margin, whereas here the penalty is applied *outside* a tolerance region:
[
\text{hinge}(z;\delta)=\mathrm{ReLU}(\delta-z),
\qquad
D_{MM}(x)\propto \mathrm{ReLU}(z-\delta).
]
Squaring (D_{MM}) yields a squared hinge penalty, which is commonly used in support vector machines and large-margin classifiers for smoother optimization [Cortes & Vapnik, 1995; Rosasco et al., 2004]. From this perspective, the Gaussian Mesa Function can be viewed as the Boltzmann distribution associated with a squared hinge energy: points inside the margin have zero energy, and probability mass decays smoothly as violations increase.

This connection is important conceptually. It shows that mesa behavior is not an exotic construction but a familiar object in statistical learning theory, expressed in a probabilistic rather than purely optimization-based form.

#### Why this formulation is a "Rosetta Stone"

The univariate Mesa Mahalanobis Distance plays a unifying role. It provides a single algebraic object that connects:

* **Classical statistics**, via Mahalanobis distance and Gaussian densities [Mahalanobis, 1936];
* **Signal processing**, via Gaussian Mesa Functions with flat tops and Gaussian tails [Dubois et al., 2007];
* **Margin-based learning**, via hinge and squared-hinge penalties [Cortes & Vapnik, 1995];
* **Neural networks**, via ReLU activations as one-sided distance operators [Nair & Hinton, 2010].

All of these domains use superficially different language—distance, margin, activation, penalty—but they share the same underlying structure once written in terms of rectified deviations. In this sense, the mesa distance acts as a "Rosetta Stone": it translates between statistical, geometric, and neural interpretations without changing the underlying mathematics.

This univariate case is intentionally simple. Its value lies not in expressivity, but in clarity. In the sections that follow, we show that the same construction scales naturally to multivariate and polyhedral settings, where the mesa becomes a hyperrectangle or a general convex polytope, and where the connection to ReLU networks becomes explicit rather than suggestive.

---

## 4. Multivariate Mesa Gaussians

### 4.1 Review: Multivariate Mahalanobis Distance

We briefly review the multivariate Mahalanobis distance to establish notation and to make explicit the decomposition that enables the mesa generalization.

Let (\mathbf{x} \in \mathbb{R}^n) be a random vector with mean (\boldsymbol{\mu}) and positive definite covariance matrix (\Sigma \in \mathbb{R}^{n\times n}). The squared Mahalanobis distance is defined as
[
D_M^2(\mathbf{x})

(\mathbf{x}-\boldsymbol{\mu})^\top \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu}),
]
and the associated Gaussian density is
[
G(\mathbf{x}) = A \exp!\left(-\tfrac{1}{2} D_M^2(\mathbf{x})\right),
]
where (A) is the normalization constant [Mahalanobis, 1936].

#### PCA decomposition

Because (\Sigma) is symmetric and positive definite, it admits an eigendecomposition
[
\Sigma = V \Lambda V^\top,
]
where (V=[\mathbf{v}_1,\ldots,\mathbf{v}_n]) is an orthogonal matrix of eigenvectors and (\Lambda=\mathrm{diag}(\lambda_1,\ldots,\lambda_n)) contains the corresponding eigenvalues. This is the standard principal component analysis (PCA) representation [Jolliffe, 2002].

Defining coordinates in the PCA basis,
[
z_i = \mathbf{v}*i^\top(\mathbf{x}-\boldsymbol{\mu}),
]
the squared Mahalanobis distance decomposes as
[
D_M^2(\mathbf{x}) = \sum*{i=1}^n \frac{z_i^2}{\lambda_i}.
]
This expression makes explicit that the Mahalanobis distance is simply the Euclidean distance after whitening: each component is centered, decorrelated, and scaled by the inverse of its variance.

#### Component-wise distances

It is often useful to view the Mahalanobis distance as a norm of component-wise contributions. Defining
[
D_{M,i} = \frac{|z_i|}{\sqrt{\lambda_i}},
]
we can write
[
D_M(\mathbf{x}) = \left| (D_{M,1}, D_{M,2}, \ldots, D_{M,n}) \right|*2.
]
Each (D*{M,i}) measures the standardized deviation of (\mathbf{x}) from the mean along a single principal axis. Geometrically, the level sets (D_M(\mathbf{x}) = c) are ellipsoids aligned with the eigenvectors of (\Sigma), with axis lengths proportional to (\sqrt{\lambda_i}).

This component-wise view is crucial for what follows. It shows that the multivariate Mahalanobis distance is not an irreducibly global object, but a sum of independent one-dimensional deviations in a suitably chosen coordinate system. In the next subsection, each of these absolute deviations will be decomposed into one-sided terms and modified with a mesa gap, yielding a multivariate flat-topped distribution whose plateau geometry is explicit.

### 4.2 Rectified Component Distances

The decomposition in Section 4.1 expresses the Mahalanobis distance as a norm over absolute deviations along orthogonal principal axes. Each absolute value can be further decomposed into *one-sided* components, making explicit the half-space structure that will later generalize to polyhedral constraints.

#### One-sided decomposition

Using the identity
[
|z| = \mathrm{ReLU}(z) + \mathrm{ReLU}(-z),
]
each standardized component distance
[
D_{M,i} = \frac{|z_i|}{\sqrt{\lambda_i}}
]
can be written as the sum of two rectified terms:
[
D_{M,i,0} = \frac{1}{\sqrt{\lambda_i}},\mathrm{ReLU}(z_i),
\qquad
D_{M,i,1} = \frac{1}{\sqrt{\lambda_i}},\mathrm{ReLU}(-z_i).
]
Geometrically, these two terms correspond to distances from the hyperplane
(\mathbf{v}*i^\top(\mathbf{x}-\boldsymbol{\mu})=0)
on opposite sides. For any fixed (i), at most one of (D*{M,i,0}) or (D_{M,i,1}) is nonzero, since (z_i) cannot be simultaneously positive and negative.

Collecting all such half-components, the Mahalanobis distance can be expressed as an (\ell_2) norm in a (2n)-dimensional nonnegative space:
[
D_M(\mathbf{x})

\left|
(D_{M,1,0},D_{M,1,1},\ldots,D_{M,n,0},D_{M,n,1})
\right|_2.
]
Because of the disjoint-support property, this representation is exactly equivalent to the standard formulation in Section 4.1: squaring and summing simply recovers (\sum_i z_i^2/\lambda_i).

#### Equivalence when (\delta=0)

This rectified representation does not yet introduce any new geometry; it is merely a re-expression of the classical distance. When no mesa gap is present ((\delta=0) in the univariate case, or equivalently no shift of the activation thresholds), the combination of positive and negative half-components exactly reproduces the usual Mahalanobis distance. In particular,
[
\mathrm{ReLU}(z_i)+\mathrm{ReLU}(-z_i)=|z_i|
\quad\Rightarrow\quad
D_M(\mathbf{x})=\sqrt{\sum_i \frac{z_i^2}{\lambda_i}}.
]

This observation is important for two reasons. First, it shows that the rectified, half-space formulation is not an approximation: it is algebraically identical to the standard Mahalanobis distance in the zero-gap limit. Second, it provides a clean pathway for generalization. Once each axis has been split into independent half-space distances, introducing mesa gaps or relaxing orthogonality becomes a matter of modifying these components, rather than redefining the distance from scratch.

In the next subsection, we introduce per-axis gaps that create a multivariate plateau, yielding a multivariate mesa Gaussian whose geometry is no longer ellipsoidal but box-shaped in principal-component space.

### 4.3 Hyperrectangular plateaus

Introducing per-axis mesa gaps transforms the geometry of the multivariate Gaussian in a fundamental way. Instead of a single ellipsoidal mode centered at the mean, the distribution acquires a flat plateau whose shape is determined independently along each principal axis.

Let (z_i = \mathbf{v}*i^\top(\mathbf{x}-\boldsymbol{\mu})) denote the coordinate of (\mathbf{x}) along the (i)-th principal direction. Introducing a gap (\delta_i \ge 0) yields the rectified component distances
[
D*{M,i,0} = \frac{1}{\sqrt{\lambda_i}}\mathrm{ReLU}(z_i - \delta_i),
\qquad
D_{M,i,1} = \frac{1}{\sqrt{\lambda_i}}\mathrm{ReLU}(-z_i - \delta_i).
]
Both components vanish whenever (|z_i| \le \delta_i). Consequently, the total mesa distance is zero if and only if this condition holds simultaneously for all (i).

The plateau region therefore takes the form
[
\mathcal{P}

\left{
\mathbf{x} \in \mathbb{R}^n ;:;
|z_i| \le \delta_i ;; \text{for all } i
\right}.
]
In principal-component coordinates, (\mathcal{P}) is a hyperrectangle with half-widths (\delta_1,\ldots,\delta_n). In the original input space, this hyperrectangle is rotated by the eigenvector matrix (V), yielding an oriented box centered at (\boldsymbol{\mu}). This stands in contrast to the ellipsoidal level sets of the standard multivariate Gaussian and makes explicit that the mesa construction replaces radial symmetry with axis-aligned bounds in a learned coordinate system.

Outside the plateau, deviations along each axis are aggregated to produce the overall distance. Using an (\ell_2) aggregation yields
[
D_{MM}(\mathbf{x})

\left|
(D_{M,1,0},D_{M,1,1},\ldots,D_{M,n,0},D_{M,n,1})
\right|*2,
]
which recovers Gaussian-like decay while smoothing the corners of the hyperrectangle. An alternative is (\ell_1) aggregation,
[
D*{MM}^{(L1)}(\mathbf{x})

\sum_{i=1}^n (D_{M,i,0}+D_{M,i,1}),
]
which produces sharper, diamond-shaped level sets.

A key observation is that the two half-components associated with any given axis have disjoint support: at most one of (D_{M,i,0}) or (D_{M,i,1}) can be nonzero. As a result, when only a small number of axes violate their bounds, the numerical difference between (\ell_1) and (\ell_2) aggregation is limited, even in high dimensions. This disjoint-support property explains why rectified, axis-wise distance constructions can behave stably despite the usual concentration phenomena associated with high-dimensional norms [Vershynin, 2018].

Geometrically, the multivariate mesa Gaussian can be viewed as the Minkowski sum of a hyperrectangle (the plateau) and an ellipsoid (the Gaussian decay), producing a "rounded box" shape. This geometry will later generalize naturally when orthogonality is relaxed and the plateau is defined by arbitrary half-space constraints.

### 4.4 Interpretation

The multivariate mesa Gaussian should be understood as a **bounded, variance-weighted deviation model**, rather than as a mixture of local modes. The plateau defines a region of zero deviation—a set of points that are all equally typical under the model. Probability mass is not concentrated at a single mean but spread uniformly across this region, with Gaussian-like decay only once a boundary is crossed.

This structure differs fundamentally from mixture models. In a Gaussian mixture, probability is additive: mass is distributed across multiple peaks, and different components compete to explain each observation. In the mesa construction, probability is multiplicative: each axis contributes an independent penalty when its corresponding bound is violated. The plateau emerges from the simultaneous satisfaction of all constraints, not from averaging over alternatives. This aligns the model more closely with product-of-experts constructions than with mixtures [Hinton, 2002].

From a statistical perspective, the multivariate mesa Gaussian can also be viewed as a natural extension of two-piece and skew-normal distributions. Classical two-piece normals allow different variances on either side of a mode while preserving continuity at the center [Arellano-Valle et al., 2005]. The mesa construction generalizes this idea in two directions: first, by introducing a finite-width region of zero curvature at the mode; second, by extending the notion of asymmetric scaling independently along multiple axes. The result is a distribution whose mode is not a point but a convex region, while still retaining Gaussian tails.

Importantly, none of these interpretations require viewing the model as defining literal probabilities in isolation. In many applications, including neural network analysis, the normalization constant is irrelevant, and only relative energies or distances matter. In this sense, the multivariate mesa Gaussian functions as an energy landscape whose geometry encodes acceptable regions and graded penalties for deviation, rather than as a fully specified generative model.

---

## 5. Polyhedral Mesa Gaussians

### 5.1 From orthogonal axes to arbitrary constraints

The multivariate mesa construction in Section 4 relies on a PCA decomposition, which provides a convenient orthogonal basis in which component-wise deviations are independent and easily interpreted. Orthogonality, however, is not essential to the underlying idea. What matters is not that deviations are measured along orthogonal axes, but that they are measured relative to a set of learned constraints.

To generalize beyond PCA, we replace principal axes with an arbitrary collection of hyperplane normals ({\mathbf{n}*j}*{j=1}^m \subset \mathbb{R}^n). Each normal defines a linear constraint of the form
[
\mathbf{n}_j^\top \mathbf{x} = \mu_j,
]
together with an associated one-sided deviation measured by a rectified distance. Geometrically, this shifts the perspective from coordinate-aligned bounds in a decorrelated space to constraint-defined regions in the original input space.

This relaxation removes the requirement that the constraint directions be mutually orthogonal or even linearly independent. The resulting construction no longer corresponds to a true Mahalanobis distance in the strict metric sense, since correlated constraints may penalize the same deviation multiple times. Nevertheless, it preserves the essential structure of the mesa: a region in which all constraints are satisfied, and a smooth, monotonic penalty for violating one or more of them.

From a modeling standpoint, this step is crucial. Real data rarely admits a single global orthogonal basis in which all relevant bounds are axis-aligned. Allowing arbitrary hyperplane orientations makes it possible to represent oblique, overlapping, or redundant constraints, while retaining a convex plateau defined by their intersection. When the normals are orthogonal and paired, the construction reduces exactly to the multivariate mesa Gaussian of Section 4; when they are not, it yields a more general polyhedral mesa geometry.

This shift from axes to constraints also anticipates the connection to ReLU networks developed later in the paper. Individual ReLU units define half-spaces with arbitrary orientations, not principal components. Interpreting them as distance-to-constraint operators therefore requires precisely this relaxation from orthogonality to general hyperplane structure.

### 5.2 Definition of Polyhedral Mesa Distance

With orthogonality relaxed, the mesa construction can be defined directly in terms of half-space constraints. Each constraint corresponds to a hyperplane together with a one-sided distance that activates only when the constraint is violated.

Let ({\mathbf{n}*j}*{j=1}^m \subset \mathbb{R}^n) be a collection of (not necessarily orthogonal) normal vectors, with offsets (\mu_j \in \mathbb{R}), plateau widths (\delta_j \ge 0), and scale parameters (\lambda_j > 0). For an input (\mathbf{x}), define the signed distance to the (j)-th hyperplane as
[
d_j(\mathbf{x}) = \mathbf{n}_j^\top \mathbf{x} - \mu_j.
]
The corresponding one-sided, variance-weighted deviation is
[
D_j(\mathbf{x})

\frac{1}{\sqrt{\lambda_j}},\mathrm{ReLU}!\left(d_j(\mathbf{x}) - \delta_j\right).
]
This quantity is zero whenever the input lies within a margin (\delta_j) of the hyperplane on the permitted side, and increases linearly once that margin is exceeded.

The plateau region is defined by the simultaneous satisfaction of all constraints:
[
\mathcal{P}

\left{
\mathbf{x} \in \mathbb{R}^n ;:;
d_j(\mathbf{x}) \le \delta_j ;; \text{for all } j = 1,\ldots,m
\right}.
]
By construction, (\mathcal{P}) is a convex polyhedron (possibly unbounded or empty), given by the intersection of half-spaces. Inside (\mathcal{P}), all component distances vanish and the total deviation is zero.

The polyhedral mesa distance aggregates these deviations,
[
D_{PM}(\mathbf{x})

\left|
\big(D_1(\mathbf{x}),\ldots,D_m(\mathbf{x})\big)
\right|*2,
]
yielding a smooth penalty away from the polytope that grows with the magnitude and number of violated constraints. Alternative aggregations, such as (\ell_1) or (\ell*\infty) norms, may also be used, but the (\ell_2) form most directly parallels Gaussian energy.

The associated polyhedral mesa Gaussian (PMG) is defined as
[
p(\mathbf{x})
\propto
\exp!\left(-\tfrac{1}{2} D_{PM}(\mathbf{x})^2\right),
]
which can be interpreted as a Boltzmann distribution with energy
[
E(\mathbf{x}) = \tfrac{1}{2} D_{PM}(\mathbf{x})^2.
]
Each constraint acts as an independent "expert" that assigns zero energy within its admissible region and a quadratic penalty outside. The resulting density is log-concave wherever the aggregation norm is convex, ensuring a single connected mode corresponding to the polytope (\mathcal{P}).

This formulation makes explicit that PMGs are not mixtures of local components but energy-based models defined by intersecting constraints. The plateau is the ground state of the energy, and deviations are penalized according to their distance from the nearest violated faces of the polytope.

### 5.3 Geometry and properties

The polyhedral mesa construction inherits a number of useful geometric and computational properties from its definition in terms of rectified half-space distances.

First, the negative log-density
[
-\log p(\mathbf{x}) = \tfrac{1}{2} D_{PM}(\mathbf{x})^2 + \text{const}
]
is a convex function of (\mathbf{x}) whenever a convex aggregation norm (such as (\ell_2) or (\ell_1)) is used. Each component (D_j(\mathbf{x})) is convex as the composition of an affine function with a ReLU, and convexity is preserved under nonnegative weighted sums and norms. As a consequence, the energy landscape has a single connected set of global minima corresponding to the plateau polytope, with no spurious local minima outside it. This places polyhedral mesa Gaussians within the class of log-concave densities, which are known to admit favorable optimization and sampling properties [Lovász and Vempala, 2007].

Second, the construction is continuous everywhere and piecewise smooth. Inside the plateau, the gradient of the energy is exactly zero. Outside the plateau, the energy is differentiable within each cell defined by a fixed set of active constraints. Non-differentiabilities occur only at the faces of the polytope, where individual ReLU components switch on or off. These kinks are identical in nature to those already present in standard ReLU networks and are well handled by subgradient-based optimization methods [Nair and Hinton, 2010].

From a computational perspective, evaluation of (D_{PM}(\mathbf{x})) requires (O(m)) operations, linear in the number of constraints and independent of the ambient dimension beyond the cost of the dot products (\mathbf{n}_j^\top \mathbf{x}). No matrix inversions or eigendecompositions are required once the constraint parameters are fixed, making the model scalable in settings where the number of constraints is moderate.

The relationship to hinge loss and support vector machines is direct. Each component (D_j(\mathbf{x})) is a scaled hinge penalty measuring violation of a margin (\delta_j). Squaring these penalties yields the squared hinge loss commonly used in large-margin classifiers and metric learning [Cortes and Vapnik, 1995; Rosasco et al., 2004]. From this viewpoint, the polyhedral mesa energy is a sum of squared hinge losses, and the resulting density is the Boltzmann distribution associated with that energy.

More broadly, this places the construction within the framework of quadratic penalty methods in constrained optimization. Hard constraints of the form (d_j(\mathbf{x}) \le \delta_j) are relaxed into soft penalties that vanish inside the feasible region and grow smoothly outside it [Nocedal and Wright, 2006]. The mesa Gaussian can therefore be interpreted as the maximum-entropy distribution consistent with approximate satisfaction of a set of linear constraints, with the penalty scales (\lambda_j) controlling the strictness of each constraint.

These connections clarify that polyhedral mesa Gaussians are not an exotic departure from existing methods, but a unifying geometric view that links ReLU nonlinearities, margin-based losses, and convex constraint modeling within a single probabilistic framework.

### 5.4 Norm choices and correlation

The definition of the polyhedral mesa distance leaves open the choice of how individual constraint violations are aggregated. This choice affects the geometry of level sets outside the plateau and determines how correlated constraints interact.

Using an (\ell_2) norm,
[
D_{PM}^{(L2)}(\mathbf{x}) = \sqrt{\sum_{j=1}^m D_j(\mathbf{x})^2},
]
produces smooth, rounded level sets and preserves the closest formal analogy to Gaussian energy. However, when multiple constraints are violated simultaneously—particularly if their normals are nearly parallel—this aggregation can over-count deviation. Points near corners of the polytope incur a larger penalty than their geometric distance to the nearest face would suggest, because correlated violations contribute additively in squared form.

An (\ell_1) aggregation,
[
D_{PM}^{(L1)}(\mathbf{x}) = \sum_{j=1}^m D_j(\mathbf{x}),
]
shares this over-counting issue but produces sharper, diamond-shaped level sets. While computationally simpler and often more robust to outliers, the (\ell_1) norm introduces additional nonsmoothness along coordinate-aligned ridges in constraint space, which may or may not be desirable depending on the application [Rosasco et al., 2004].

At the opposite extreme, an (\ell_\infty) aggregation,
[
D_{PM}^{(L\infty)}(\mathbf{x}) = \max_j D_j(\mathbf{x}),
]
penalizes only the most violated constraint. This choice aligns more closely with the true Euclidean distance to a single face of the polytope and avoids over-penalizing corners. However, it discards information about simultaneous violations and introduces non-differentiable transitions when the identity of the most violated constraint changes.

A more principled, but delicate, alternative is to account explicitly for correlations between constraints. Let (G) denote the Gram matrix of normalized constraint normals, with entries (G_{ij} = \mathbf{n}_i^\top \mathbf{n}*j). One may define a corrected distance
[
D*{\text{corr}}^2(\mathbf{x}) = \mathbf{D}(\mathbf{x})^\top (G+\varepsilon I)^{-1} \mathbf{D}(\mathbf{x}),
]
where (\mathbf{D}=(D_1,\ldots,D_m)) and (\varepsilon>0) is a regularization parameter. This reduces to the standard (\ell_2) aggregation when constraints are orthogonal and downweights redundant, highly correlated violations.

Such corrections, however, come with practical caveats. The Gram matrix may be ill-conditioned or singular when constraints are nearly dependent, and its inversion introduces additional computational cost and sensitivity to regularization. For this reason, Gram-based corrections are best viewed as a conceptual bridge rather than a default choice.

Overall, the choice of norm reflects a trade-off between geometric fidelity, smoothness, and robustness. The mesa framework is deliberately agnostic to this choice: different aggregations correspond to different assumptions about how constraint violations should interact, while preserving the core structure of a convex plateau with monotonic penalty outside it.

---

## 6. ReLU Networks as Polyhedral Mesa Compositions

### 6.1 Single neuron: distance to a hyperplane

Consider a single ReLU neuron with output
[
h(\mathbf{x}) = \mathrm{ReLU}(\mathbf{w}^\top \mathbf{x} + b).
]
This computation is often described informally as measuring similarity between the input (\mathbf{x}) and a learned template (\mathbf{w}). That interpretation, however, obscures the actual geometry of the operation. Algebraically, the neuron computes a **one-sided deviation from a hyperplane**.

The affine equation
[
\mathbf{w}^\top \mathbf{x} + b = 0
]
defines a hyperplane in input space with normal vector (\mathbf{w}). Any input (\mathbf{x}) can be decomposed into a component parallel to this normal and a component lying in the hyperplane. Along the normal direction, the scalar quantity
[
z(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b
]
is the signed coordinate of (\mathbf{x}) relative to the hyperplane. Up to a constant scale factor, this coordinate is proportional to the signed Euclidean distance from the hyperplane:
[
d(\mathbf{x}) ;\propto; \mathbf{w}^\top \mathbf{x} + b.
]

Applying the ReLU nonlinearity yields
[
h(\mathbf{x}) = \mathrm{ReLU}!\big(z(\mathbf{x})\big),
]
which is zero on one side of the hyperplane and increases linearly with signed displacement on the other. Thus, a ReLU neuron computes a **rectified, one-sided distance coordinate**: it measures how far an input lies beyond a learned linear boundary, while ignoring variation on the admissible side.

If one wishes to make the distance interpretation explicit, the scale of this deviation can be written in Mahalanobis form. Defining a one-dimensional variance parameter
[
\lambda = \frac{1}{|\mathbf{w}|^2},
]
the quantity
[
\frac{1}{\sqrt{\lambda}},\mathrm{ReLU}!\big(z(\mathbf{x})\big)
]
is a one-sided Mahalanobis distance along the normal direction to the hyperplane. Importantly, this is not an additional assumption imposed on the network: the scale parameter (\lambda) is already implicit in the weight norm. Choosing whether or not to normalize simply corresponds to choosing units for measuring deviation.

Normalization techniques such as batch normalization or weight normalization may reparameterize this scale, but they do not alter the underlying structure of the computation. In all cases, the neuron remains silent within a half-space and produces a linear penalty once a constraint is violated.

Viewed through this lens, a single ReLU neuron is not a feature detector that "fires" when an input matches a template. It is a **constraint operator**: it encodes a linear boundary and reports the magnitude of violation beyond that boundary. This one-sided, variance-weighted deviation is precisely the atomic operation used in the polyhedral mesa construction, where complex prototype regions arise from the intersection and composition of many such constraints.

### 6.2 One hidden layer: intersection of constraints

Consider a network with a single hidden layer,
[
\mathbf{h} = \mathrm{ReLU}(W\mathbf{x} + \mathbf{b}),
\qquad
y = \mathbf{a}^\top \mathbf{h} + c,
]
where (W \in \mathbb{R}^{m\times n}) and (\mathbf{a} \in \mathbb{R}^m). Each row (\mathbf{w}_j^\top) of (W) defines a hyperplane in input space, and each hidden unit computes a rectified, one-sided deviation from that hyperplane, as described in Section 6.1.

When the output weights satisfy (a_j \ge 0) for all (j), the second layer forms a nonnegative linear combination of these deviations:
[
y(\mathbf{x}) = \sum_{j=1}^m a_j, D_j(\mathbf{x}) + c,
\qquad
D_j(\mathbf{x}) = \mathrm{ReLU}(\mathbf{w}_j^\top \mathbf{x} + b_j).
]
In this regime, reducing the output requires simultaneously reducing all active deviation terms. The minimum of (y(\mathbf{x})) is therefore attained precisely on the set of inputs for which every contributing constraint is satisfied—that is, where all corresponding hidden activations vanish.

Geometrically, this means that the low-output region is the **intersection of half-spaces** defined by the selected hidden units. Each hidden neuron contributes a single linear constraint; the output neuron combines them conjunctively by summation with nonnegative weights. Adding an additional positively weighted constraint can only shrink—or at best preserve—the region of low activation; it cannot expand it. This monotonicity follows directly from convexity and mirrors the behavior of constraint intersections in convex geometry [Ziegler, 1995].

An important structural consequence is that the hyperplanes defined by the first layer function as a **shared constraint vocabulary**. A fixed collection of (m) hyperplanes can be recombined by different output units to define many distinct polyhedral regions, without requiring the network to relearn new boundaries for each output. Expressivity arises from combinatorial reuse of constraints rather than from memorization of point-like templates, providing a geometric explanation for the parameter efficiency of shallow ReLU networks.

From the perspective of the mesa framework, a one-hidden-layer ReLU network with nonnegative output weights computes a family of **polyhedral mesa energies**, one per output unit. Each output corresponds to a different intersection of shared half-space constraints, and therefore to a different prototype *region* rather than a prototype point. Decisions are made by measuring the degree of constraint violation—how far an input lies outside a feasible polytope—rather than by matching the input to a learned exemplar. This view aligns naturally with margin-based classification and energy-based modeling, where inference is driven by distance from constraint-defined sets rather than by similarity to templates.

### 6.3 Depth: composition of deviation coordinates

In deeper ReLU networks, the interpretation of activations as deviation coordinates extends recursively across layers. After the first hidden layer,
[
\mathbf{h}_1 = \mathrm{ReLU}(W_1 \mathbf{x} + \mathbf{b}_1),
]
the representation no longer resides in the original input space. Instead, it lies in a nonnegative space whose coordinates encode **violations of first-layer constraints**. Each component of (\mathbf{h}_1) measures the extent to which the input exceeds a learned half-space boundary, and vanishes when that constraint is satisfied.

A subsequent layer applies another affine transformation followed by ReLU,
[
\mathbf{h}_2 = \mathrm{ReLU}(W_2 \mathbf{h}_1 + \mathbf{b}_2).
]
This operation admits a precise two-sided interpretation. From a backward-looking perspective, the rows of (W_2) aggregate first-layer deviation coordinates into linear combinations, producing new quantities that behave like polyhedral mesa distances over the space of violations described in Section 5. From a forward-looking perspective, each row of (W_2) defines a new hyperplane—but now in the space of deviation profiles rather than in the original input space. The resulting ReLU therefore enforces a **second-order constraint**: it activates only when a particular combination of first-layer constraint violations exceeds a learned threshold.

In this sense, deeper layers do not measure distances in input space directly. Instead, they impose constraints on *patterns of deviation*. The prototype at layer (k) is a region in the representation space of layer (k-1), defined by the simultaneous satisfaction of higher-order constraints. The phrase "distance-of-distance" should therefore be understood operationally: each layer constructs rectified deviation coordinates over the outputs of the previous layer, without assuming the existence of a single global metric across layers.

Crucially, while each layer operates using convex primitives—affine maps, ReLUs, and intersections of half-spaces—the overall input-output mapping need not preserve convexity. ReLU networks partition input space into many linear regions, within each of which the network acts affinely [Montúfar et al., 2014; Balestriero and Baraniuk, 2018]. The preimage of a convex polytope in a deep layer is therefore generally a union of convex polytopes in input space. This mechanism explains how deep networks can realize highly non-convex decision regions while maintaining convex structure locally at every layer.

From the mesa perspective, depth increases expressivity by enabling constraints to be imposed on increasingly abstract summaries of deviation. Early layers define basic geometric bounds in input space; subsequent layers learn which *combinations* of bound violations are acceptable or unacceptable for the task at hand. The resulting hierarchy provides a geometric account of depth that is consistent with existing results on the expressive power of deep ReLU networks, while grounding that expressivity in a concrete, constraint-based interpretation rather than in template matching or similarity measures.

### 6.4 Interpretive inversion

The distance-based view of ReLU activations leads to a direct inversion of the standard interpretive narrative. Rather than treating activation magnitude as evidence for the presence of a feature, the mesa framework treats activation as evidence of deviation from an acceptable region [Oursland, 2024a].

In the conventional interpretation, a neuron "fires" when an input matches its preferred stimulus, and larger activations are taken to indicate stronger confidence that the feature is present [Oursland, 2024b]. This view implicitly borrows intuition from normalized similarity measures, even though the architecture itself does not enforce such normalization.

Under the mesa interpretation, the situation is reversed. A ReLU unit is quiescent when the input lies within its prototype region, defined by a half-space or an intersection of half-spaces. Activation occurs only when the input violates a learned constraint, and the magnitude of activation measures how far the input lies outside that region. Zero activation is therefore not absence of information, but maximal conformity [Oursland, 2024a].

This inversion can be summarized succinctly [Oursland, 2024a]:

* Activation does not indicate feature presence; it indicates constraint violation.
* Larger activations do not mean stronger membership; they mean greater deviation.
* Zero activation corresponds to the prototype region itself, not to an empty or inactive state.

Viewed this way, sparsity in ReLU networks acquires a natural geometric interpretation. Sparse activations mean that most constraints are satisfied and only a small number are violated. This is the expected regime for inputs that lie well within learned prototype regions, rather than an incidental byproduct of regularization.

The interpretive inversion also aligns ReLU networks with energy-based models, where low energy corresponds to typical or acceptable configurations and high energy corresponds to unlikely or anomalous ones [LeCun et al., 2006]. In the mesa framework, the "energy" of an input is precisely its accumulated deviation from a hierarchy of learned constraints.

---

## 7. Discussion

### 7.1 What the PMG Framework Provides

The Polyhedral Mesa Gaussian offers three contributions to neural network research.

**A specific geometric object.** When asking "what does this network represent?", the PMG provides a concrete answer: look for convex polytope regions where activations are minimal, with Gaussian falloff measuring deviation from these regions. This is more specific than "features" or "representations"—it predicts a particular shape (polytope), a particular structure (plateau plus tails), and particular properties (convexity, smooth falloff). These predictions are testable.

**A geometric vocabulary.** The framework introduces precise terms for discussing neural network geometry: *plateau* (the prototype region), *mesa distance* (aggregate constraint violation), *constraint satisfaction* (zero activation). This vocabulary enables questions that are difficult to formulate otherwise: What is the volume of a concept's plateau? How many faces define it? How far is this input from the nearest plateau boundary? Such questions transform interpretability from qualitative description to quantitative measurement.

**A bridge between domains.** The PMG connects neural network computation to probability theory (Gaussian distributions, Mahalanobis distance), convex geometry (polytopes, half-space intersections), optimization (convex energy functions, hinge loss), and signal processing (Gaussian Mesa Functions). These connections are not merely analogical—they are algebraic identities. A ReLU layer *is* a constraint violation computation; the PMG *is* the probability interpretation of that computation. This bridge allows tools and insights from established fields to transfer to neural network analysis.

### 7.2 Prototypes as Regions

The PMG framework implies a fundamental shift in how prototypes should be understood. Traditional neural network interpretation seeks prototype *points*—inputs that maximally activate neurons. The PMG framework seeks prototype *regions*—sets of inputs that minimally activate neurons, lying within the plateau where all constraints are satisfied.

This shift resolves a geometric pathology of point-based interpretations. ReLU activations are unbounded above: for any neuron, scaling an input along the weight direction increases activation without limit. There is no finite maximally activating input unless normalization is externally imposed. The notion of an "ideal point" that best activates the neuron is undefined in the native geometry of the model. Techniques such as activation maximization produce extreme, uninterpretable inputs precisely because the optimization problem they pose is unbounded [Erhan et al., 2009].

The mesa framework dissolves this problem. A prototype is not a point to approach but a region to remain inside. All inputs within the plateau are equally prototypical; none is preferred over another by the model. Meaning is encoded by boundaries, not centers. The identity of a concept is determined by the conjunction of constraints it satisfies, rather than by proximity to a single exemplar.

This view reframes recognition as *exclusion* rather than matching. An input belongs to a class not because it strongly activates a template, but because it survives the constraints that define that class's boundary. Classification asks which class's constraints are least violated, not which template is most similar.

### 7.3 Adversarial Vulnerability as Boundary Crossing

The PMG framework offers a geometric explanation for adversarial vulnerability. If class regions are polytopes defined by constraint intersections, then an adversarial attack is simply a trajectory that crosses the nearest constraint boundary [Szegedy et al., 2014; Goodfellow et al., 2015].

An input need not move toward a different class's prototype point; it need only exit its own class's polytope and enter another's. High-dimensional polytopes often have thin regions or sharp corners where an input may lie well inside the plateau volumetrically but close to a constraint face. A small, carefully chosen perturbation can push the input across this nearby boundary, causing the mesa distance to spike for the original class while dropping for an adversary class.

This boundary-crossing view explains several empirical observations. Adversarial perturbations are typically small in norm but highly structured—they are optimized to cross the nearest face, not to traverse large distances. The DeepFool algorithm [Moosavi-Dezfooli et al., 2016] explicitly computes the shortest path to a decision boundary under the assumption that boundaries are locally flat (polyhedral), and its success confirms that trained networks do exhibit this geometry.

Robustness, in this view, is governed by *margin to the nearest constraint surface* rather than by distance between class centroids. Networks with wider plateau regions or larger gaps between class polytopes should be more robust; networks with thin, elongated polytopes or boundaries that pass close to typical inputs should be more vulnerable.

### 7.4 Limitations

**Negative weights.** The PMG interpretation is cleanest when aggregation weights are non-negative, implementing constraint intersection. Negative weights introduce set subtraction and relative comparisons, creating non-convex regions that the basic PMG cannot represent. A complete geometric account of negative weights—likely involving difference-of-convex (DC) decomposition [Tao & An, 1997]—remains open. Trained networks use negative weights extensively, so this gap limits the framework's direct applicability.

**Normalization.** The PMG normalization constant $A$ is intractable for complex polytopes, requiring integration over the polytope interior and exterior regions. This prevents exact probabilistic inference (computing posterior probabilities, sampling). However, for discrimination and ranking—determining whether a point is inside or outside, or which of several polytopes it is nearest to—normalization is unnecessary. The unnormalized energy $D_{PM}^2$ suffices. This aligns with energy-based model methodology [LeCun et al., 2006].

**Empirical validation.** This paper establishes the PMG as a mathematical framework; it does not empirically verify that trained networks learn PMG-structured representations. Prior work supports the constituent claims: networks use distance metrics [Oursland, 2024b], networks learn polytope partitions [Park et al., 2024; Fan et al., 2023], mesa shapes arise in biological signal modeling [Dubois et al., 2007]. But direct measurement of PMG structure in trained networks—identifying plateau regions, measuring falloff rates, comparing to predictions—remains future work.

**Single-component limitation.** A single PMG has a convex plateau. Representing non-convex concept regions requires mixtures of PMGs or hierarchical composition through network depth. The framework describes what one "mesa" looks like; how multiple mesas combine to represent complex categories is not fully specified.

### 7.5 Future Directions

The PMG framework suggests several research directions. Empirically: measure plateau geometry in trained networks, test whether activation distributions match PMG predictions, compare network robustness to plateau-boundary versus plateau-interior perturbations. Theoretically: extend the framework to handle negative weights, characterize how depth enables non-convex regions through PMG composition, establish connections to tropical geometry and max-affine splines [Balestriero & Baraniuk, 2018]. Practically: explore whether architectures explicitly designed around PMG structure (following the OffsetL2 approach in [Oursland, 2025]) offer advantages in interpretability or robustness.

The PMG does not claim to be a complete theory of neural network computation. It claims to be a useful lens—one that makes specific predictions, connects to established mathematics, and provides vocabulary for precise questions. Its value will be determined by whether that lens reveals structure that other frameworks miss.

---

## 8. Empirical Evidence and Open Predictions

### 8.1 Strong support: polytope structure

The strongest empirical support for the Polyhedral Mesa Gaussian framework lies in the now well-established observation that ReLU networks induce polytope structure throughout representation space. This support comes from multiple, largely independent lines of work spanning theory, empirical analysis, and interpretability.

First, polytope structure is a mathematical consequence of the ReLU nonlinearity itself. A ReLU network partitions its input space into a finite collection of convex polytopes, within each of which the network computes an affine function. Each ReLU introduces a hyperplane that splits space into active and inactive half-spaces, and the intersection patterns of these half-spaces determine the polyhedral regions [Montúfar et al., 2014; Pascanu et al., 2014; Serra et al., 2018]. This is not a modeling assumption but a definitional property of piecewise-linear networks.

Second, empirical studies show that trained networks converge to *surprisingly simple* polytopes. Although deep ReLU networks are theoretically capable of generating extremely complex partitions, gradient-based training strongly biases solutions toward low-complexity polyhedral structures. Fan et al. [2023] demonstrate that both randomly initialized and trained networks exhibit relatively few, large polytopes compared to worst-case constructions, suggesting an implicit geometric regularization effect during optimization.

Third, recent work has shown that real-world datasets can be efficiently covered by small collections of polytopes. Lee et al. [2024] introduce the notion of polytope-basis covers and show that standard image classification datasets such as MNIST, Fashion-MNIST, and CIFAR-10 can be encapsulated using only one or two polytopes per class, each with a small number of faces. They further show that shallow ReLU architectures naturally learn decision boundaries consistent with these polytope covers. This provides direct evidence that learned class regions are bounded, convex objects rather than unbounded half-spaces or scattered point clusters.

Fourth, polytope structure has been observed at the semantic level in large language models. Park et al. [2024] analyze internal representations of LLMs and find that simple categorical concepts are represented as simplices—the minimal convex polytopes—while more complex concepts are represented as higher-order polytopes constructed from combinations of simpler ones. This result is particularly notable because simplices and polytopes are exactly the geometric objects predicted by constraint-based, intersection-driven representations, rather than by point-prototype or mixture-based models.

Taken together, these results provide strong, convergent evidence that polytope geometry is not an incidental artifact of ReLU networks, but a dominant organizing principle of their learned representations. The Polyhedral Mesa Gaussian framework builds directly on this foundation, offering a distance-based and probabilistic interpretation of these empirically observed polytope regions rather than introducing a new geometric hypothesis.

### 8.2 The two-sided bound (antipodal) hypothesis

Beyond generic polytope structure, the Polyhedral Mesa Gaussian framework makes a **specific and falsifiable geometric prediction**. The claim is conditional, not universal:

> **When a learned latent dimension is bounded on both sides, that bounded interval is implemented by two opposing half-space constraints with approximately antipodal normals.**

In concrete terms, if a representation encodes that a scalar degree of freedom must remain within an acceptable range—"not too small" and "not too large"—then the simplest realization in a ReLU network is a pair of hyperplanes whose normals point in opposite directions and whose offsets jointly define that interval.

This prediction concerns **how bounds are implemented**, not how concepts are opposed.

#### What the hypothesis predicts, precisely

The hypothesis does **not** assert that semantic opposites (e.g., *happy* vs. *sad*, *dog* vs. *cat*) should be represented by antipodal vectors. Instead, it predicts the following narrowly defined structural pattern:

* For a given latent scalar direction (z), the network learns **two threshold constraints**:

  * one that activates when (z) exceeds an upper bound,
  * one that activates when (z) falls below a lower bound.
* Algebraically, this corresponds to an approximate realization of
  [
  |z-\mu| ;\approx; \mathrm{ReLU}(z-\mu) + \mathrm{ReLU}(\mu-z),
  ]
  implemented by two ReLU units with weight vectors of opposite orientation.
* Geometrically, these two units define **parallel faces of the same polytope**, jointly bounding a single acceptable region rather than encoding competing categories.

The prediction is therefore about **constraint pairs**, not semantic polarity. Antipodal normals arise only when the representation enforces a *two-sided tolerance* along a shared latent dimension. If a feature is bounded on only one side—as in simplex-like or minimal polytopes—no antipodal pair is required or expected.

This also implies a layer dependence. The hypothesis is most likely to hold in early or intermediate layers that encode continuous attributes such as magnitude, intensity, count, or degree. Later layers, which often represent categorical abstractions with minimal closure, may favor simplex-like geometry that does not require symmetric bounds.

#### Why semantic antonym tests miss the prediction

Recent empirical work analyzing feature geometry in sparse autoencoders reports that semantic antonyms are generally **not** antipodal in representation space, and that antipodal directions, when observed, are often semantically unrelated [Li et al., 2025]. Taken at face value, this appears to contradict the antipodal hypothesis—but it tests a different question.

Semantic antonyms are *categorical alternatives*, not upper and lower bounds on a shared latent variable. There is no reason for "happy" and "sad" to form opposite faces of a single acceptable region; they correspond to distinct regions altogether. The two-sided bound hypothesis instead concerns relationships of the form:

* "too bright" vs. "too dark" bounding a normal illumination range,
* "too many" vs. "too few" bounding an acceptable count,
* "overconfident" vs. "underconfident" bounding a calibrated response.

These are **two-sided constraints on the same underlying degree of freedom**, not semantic negations. Testing for antipodality by pairing human-interpretable antonyms therefore conflates categorical opposition with geometric bounding and is largely insensitive to the structure the mesa framework predicts.

Evidence consistent with the hypothesis does appear in settings where the data explicitly enforce anticorrelation along a single axis. Toy models with constructed anticorrelated features reliably produce antipodal storage in learned representations [Anthropic, 2022; LessWrong, 2024], and analyses of training dynamics show that opposite-signed features can coexist stably due to ReLU clipping ("benign collisions") [ICLR Workshop, 2024]. These results do not establish that real networks systematically learn two-sided bounds, but they demonstrate that the mechanism is both available and stable when the data demand it.

In summary, the two-sided bound hypothesis makes a narrow and testable claim:

> **If a network learns that a latent dimension is bounded on both sides, antipodal constraint pairs are the natural geometric implementation.**

Existing empirical studies have not directly tested this claim because they have focused on semantic opposition rather than paired thresholds along a shared latent dimension. This gap motivates targeted empirical analyses that search explicitly for matched upper-lower constraint pairs, rather than for semantic antonyms, in learned representations.

### 8.3 Current evidence: supportive and negative

Empirical evidence bearing on the Polyhedral Mesa Gaussian framework separates cleanly into two categories: strong support for polytope structure in general, and mixed, incomplete evidence regarding the more specific two-sided bound (antipodal) hypothesis.

Supportive evidence for the antipodal mechanism appears most clearly in controlled or simplified settings. In toy models with explicitly anticorrelated features, both dense networks and sparse autoencoders reliably learn antipodal weight vectors to represent upper and lower deviations along a single latent axis [Anthropic, 2022; LessWrong, 2024]. In these cases, the data distribution itself enforces a bounded, one-dimensional structure, and the learned geometry reflects this directly. These results demonstrate that antipodal constraint pairs are a natural and stable solution when the learning problem requires them.

Additional indirect support comes from analyses of training dynamics and feature superposition. Work on incidental polysemanticity shows that features with opposite-signed weights can coexist without destructive interference because ReLU clipping suppresses negative overlap ("benign collisions") [ICLR Workshop, 2024]. While this phenomenon was not studied in the context of bounded dimensions, it establishes that antipodal directions are not penalized by optimization and can persist through training.

Evidence from equivariance studies also points toward structured symmetry in learned representations. Vision models routinely learn feature families related by rotations, reflections, or sign flips, with excitation in one direction and inhibition in the opposite [Olah et al., 2020]. Although these symmetries are typically interpreted in terms of orientation or phase rather than bounding, they indicate that neural networks readily organize features into geometrically meaningful, paired structures when the data exhibit the corresponding symmetry.

In contrast, the strongest negative evidence comes from analyses of sparse autoencoder feature geometry applied to large language models. These studies find that semantically opposite features are generally not antipodal in representation space, and that antipodal directions, when present, often lack clear semantic correspondence [Li et al., 2025]. As discussed in Section 8.2, this result challenges a naive "antonym-as-opposite-vector" interpretation but does not directly address the mesa hypothesis, which concerns paired constraints on a shared latent dimension rather than categorical alternatives.

Taken together, the current evidence supports a cautious conclusion. Polytope structure is robust and ubiquitous, and antipodal constraint pairs clearly arise when the data impose bounded, anticorrelated structure. At the same time, there is no evidence that networks systematically arrange high-level semantic features into antipodal pairs, nor does the mesa framework require this. The relevant empirical question remains open: whether networks learn paired upper-lower bounds along individual feature dimensions in realistic settings. Addressing this question requires targeted analyses designed to detect constraint pairs rather than semantic opposites.

### 8.4 What has not been tested

Despite extensive work on the geometry of ReLU networks and learned representations, several predictions specific to the Polyhedral Mesa Gaussian framework remain **empirically untested**. Importantly, these gaps are methodological rather than evidential: existing analyses have largely not been designed to measure the structures the framework predicts.

First, there has been no direct test for **paired thresholds along the same latent dimension**. Most empirical studies analyze neurons or feature directions in isolation, or compare semantically labeled directions across a representation space. The mesa framework instead predicts *within-dimension pairing*: two constraints that activate on opposite sides of the same underlying scalar degree of freedom. Detecting this structure requires identifying candidate latent directions and explicitly testing whether distinct units implement complementary upper and lower activation thresholds along that same direction. Existing analyses rarely condition on shared latent structure and therefore cannot detect this pattern even if it is present.

Second, there has been no systematic search for **matched offsets coupled with antipodal normals**. The antipodal hypothesis is not merely angular: it predicts that opposing weight vectors should be accompanied by approximately aligned bias terms, such that the two units jointly define a bounded interval. Most geometric studies focus on angular relationships between weight vectors or subspaces [Li et al., 2025; Park et al., 2024], but do not jointly analyze direction, bias, and activation statistics. As a result, they cannot determine whether two neurons form a coherent upper-lower bound pair or are merely incidentally opposed in direction.

Third, the hypothesis of **local whitening within polytopes** has not been empirically evaluated. While Mahalanobis-like structure motivates the framework, existing work typically tests for global orthogonality or global decorrelation of features. The mesa framework predicts neither. Instead, it predicts that *conditional on an activation pattern*—that is, within a single polyhedral region—the active constraints may collectively approximate a local whitening transform, decorrelating and variance-normalizing the data restricted to that region. Testing this requires conditioning on activation masks and measuring covariance locally, a level of granularity absent from current representation-geometry studies.

These omissions matter because many negative results in the literature concern **global feature geometry or semantic opposition**, which are orthogonal to the mesa predictions. The failure to observe antipodal semantic features does not falsify the framework; it reflects that the relevant hypotheses—paired thresholds, aligned offsets, and local Mahalanobis structure—have simply not been the target of empirical measurement.

In short, the current empirical landscape does not rule out the Polyhedral Mesa Gaussian framework. It leaves a clearly defined set of geometric questions unanswered, each of which can be addressed with targeted analyses on existing trained models, without requiring new architectures or training procedures.

---

## 9. Discussion

### 9.1 Box vs simplex geometry

The Polyhedral Mesa Gaussian framework predicts *bounded* regions defined by intersections of half-spaces, but it does not require those regions to be hyperrectangles with symmetric, antipodal faces. This distinction is essential for reconciling the theory with empirical observations.

A **box (hyperrectangle)** geometry arises when a latent dimension is explicitly bounded above and below by two approximately antipodal constraints. This corresponds to learning both "too much" and "too little" limits along the same degree of freedom. Such geometry is natural when the data distribution is roughly symmetric along that dimension or when task constraints explicitly demand tolerance bands. In this case, antipodal normals are an efficient representation.

A **simplex** geometry, by contrast, represents *minimal closure*. A simplex is the convex polytope with the fewest possible faces required to bound a region. In (d) dimensions, this requires only (d+1) constraints, none of which need to be antipodal. Empirically, several studies report simplex-like structures in learned representations, particularly for categorical concepts [Park et al., 2024]. From the mesa perspective, this reflects a model that bounds a concept only as much as necessary to separate it from others, without imposing redundant symmetric constraints.

This distinction explains why antipodal pairs are **optional rather than required**. The mesa framework predicts closure (a bounded acceptable region), not symmetry. Antipodal normals appear only when the data or task benefits from symmetric tolerance around a latent dimension. When such symmetry is unnecessary, the network may instead learn a simplex-like polytope, whose faces "lean" against each other to close the region without forming parallel pairs.

This also aligns with observations that trained networks tend toward **geometrically simple polytopes** despite having the capacity for more complex ones [Fan et al., 2023]. Regularization, optimization bias, and data geometry all favor minimal constraint sets. From this perspective, the frequent appearance of simplices is not evidence against the mesa framework, but evidence that networks economize on constraints.

In short, antipodal normals correspond to *redundant but symmetric* bounds, while simplices correspond to *minimal but sufficient* bounds. The Polyhedral Mesa Gaussian framework accommodates both: boxes when symmetry is useful, simplices when it is not.

### 9.2 Implications

The Polyhedral Mesa Gaussian perspective has several implications for how we understand robustness, representation, and interpretation in ReLU networks. These implications follow directly from the geometry of bounded regions and deviation-based activations, rather than from any particular training objective.

**Adversarial examples.**
If representations are defined by proximity to polytope boundaries rather than similarity to point prototypes, adversarial vulnerability becomes easier to interpret. An input need not move toward a different class prototype; it need only cross a nearby constraint boundary. Small, carefully chosen perturbations can therefore induce large changes in classification by exiting one mesa and entering another, even when the input remains perceptually similar. This boundary-crossing view aligns with empirical findings that adversarial perturbations are often small in norm but highly structured [Szegedy et al., 2014; Goodfellow et al., 2015], and suggests that robustness is governed by the *margin to the nearest constraint surface* rather than by distance between class centroids.

**Sparsity.**
Sparse activations arise naturally under the mesa interpretation. Typical inputs lie well within many prototype regions and therefore satisfy most constraints, producing zero activation. Only a small subset of constraints—those nearest to violation—activate. Sparsity is thus not primarily a regularization artifact or a coding objective, but a geometric consequence of representing concepts as bounded regions in high-dimensional space. This perspective is consistent with classic results in sparse coding and modern observations of sparse feature usage in deep networks [Olshausen and Field, 1996; Hanin and Rolnick, 2019].

**Interpretability.**
Interpreting neurons as constraint-violation detectors shifts the interpretability question from "what feature does this neuron detect?" to "what boundary does this neuron enforce?" The meaningful object is no longer a maximally activating input, but the shape and orientation of the hyperplane and its relationship to other constraints. Concepts are expressed not by single neurons but by the intersection of many such constraints. This aligns with recent interpretability work emphasizing geometric and polytope-based views of representation over single-neuron semantics [Black et al., 2022; Montúfar et al., 2014].

**Energy-based modeling.**
Finally, the mesa framework places standard ReLU networks squarely within the family of energy-based models. Activations contribute additively to an energy function, with low energy corresponding to inputs that satisfy learned constraints. The Polyhedral Mesa Gaussian makes this explicit by interpreting squared ReLU violations as quadratic penalties and the network output as an unnormalized log-density. From this perspective, classification, anomaly detection, and representation learning all operate by shaping an energy landscape with flat basins (mesas) and rising walls, without requiring explicit probability normalization [LeCun et al., 2006].

Taken together, these implications suggest that many observed properties of deep networks—adversarial sensitivity, sparsity, distributed representation, and compatibility with energy-based views—are natural consequences of polyhedral, distance-based geometry, rather than independent phenomena.

### 9.3 Limitations

This work has several important limitations that should be made explicit to avoid over-interpretation of the proposed framework.

First, the Polyhedral Mesa Gaussian perspective is **interpretive rather than prescriptive**. It does not propose a new architecture, training objective, or optimization method, nor does it claim that existing networks explicitly optimize a Mesa Gaussian likelihood. The framework provides a geometric and probabilistic *reading* of what standard linear layers with ReLU activations compute, not a statement about designer intent or training dynamics.

Second, the paper provides **no direct empirical validation** of the most specific predictions of the framework. While there is strong theoretical and empirical support for polytope structure in ReLU networks, the distinctive mesa-related signatures—such as paired upper-lower constraints along the same latent dimension or locally Mahalanobis-like behavior inside activation regions—have not yet been directly tested. As a result, the framework should be viewed as a hypothesis-generating lens rather than a confirmed model of learned representations.

Third, the framework does **not claim universality**. Not all networks, tasks, or layers need realize all aspects of the Polyhedral Mesa Gaussian construction. In particular, bounded "box-like" geometries with antipodal constraints are optional, not mandatory. As discussed in the previous section, networks may instead learn simplex-like or otherwise minimal polytopes when symmetry or two-sided bounds are unnecessary. Depth, data geometry, regularization, and task structure all influence which special cases—if any—are instantiated in practice.

Finally, the correspondence between ReLU networks and Mesa Gaussians is exact at the level of algebraic structure but approximate at the level of probabilistic interpretation. Activations should not be read as calibrated probabilities, and the implied densities are generally unnormalized. The value of the framework lies in its geometric clarity and unifying perspective, not in claiming probabilistic optimality.

Taken together, these limitations clarify the intended scope of the paper: it offers a coherent geometric interpretation of existing models, identifies concrete open questions, and motivates targeted empirical tests, without claiming completeness or empirical closure.

---

## 10. Conclusion

This paper advances a simple but unifying insight: **ReLU networks naturally define distance-based, polyhedral prototype regions**. A linear transformation followed by a ReLU does not inherently compute similarity to a template; it computes a one-sided deviation from a hyperplane. When such operations are composed across neurons and layers, the result is a hierarchy of bounded or semi-bounded regions in representation space—polytopes defined by learned constraints. Interpreted probabilistically, these structures correspond to *mesa-shaped* energy landscapes: flat regions of acceptability surrounded by smoothly increasing penalty.

The primary contribution of this work is not architectural or algorithmic, but **clarificatory**. It makes explicit a geometry that is already implicit in standard ReLU networks, connecting several strands of prior work that are often treated separately: Gaussian mesa functions in signal processing, Mahalanobis distance and whitening, hinge-loss-based constraints, polytope partitions of input space, and energy-based interpretations of neural computation [Dubois et al., 2007; Mahalanobis, 1936; Cortes and Vapnik, 1995; Montúfar et al., 2014; LeCun et al., 2006]. The Polyhedral Mesa Gaussian framework provides a single lens through which these ideas can be viewed as instances of the same underlying structure.

Crucially, the framework reframes representation learning in ReLU networks as **learning boundaries rather than exemplars**. Prototypes are regions, not points; activations measure deviation, not presence; and sparsity reflects widespread constraint satisfaction rather than selective feature detection. This perspective aligns naturally with empirical findings that concepts are represented as polytopes or simplices in trained networks, and with the growing use of distance- and energy-based reasoning in modern models [Fan et al., 2023; Park et al., 2024].

The paper closes with an invitation rather than a claim of completion. The Polyhedral Mesa Gaussian perspective generates **concrete, testable empirical questions**—most notably whether trained networks learn paired upper-lower bounds along latent dimensions, and whether local representations exhibit approximate Mahalanobis structure within activation regions. Answering these questions does not require new architectures, only new analyses. If successful, such work would not change what neural networks compute, but would sharpen our understanding of *how* and *why* their geometry supports generalization, robustness, and interpretability.

---

## Bibliography

* "Sparse autoencoders find composed features in small toy models." (2024). LessWrong.
* "Toy Models of Superposition." (2022). Anthropic.
* "What Causes Polysemanticity?" (2024). ICLR Workshop.
* Arellano-Valle, R. B., Gómez, H. W., & Quintana, F. A. (2005). Statistical inference for a general class of asymmetric distributions. *Journal of Multivariate Analysis*, 92(1), 1-24.
* Badilini, F., Vaglio, M., Dubois, R., Roussel, P., Sarapa, N., Denjoy, I., Extramiana, F., & Maison-Blanche, P. (2008). Automatic analysis of cardiac repolarization morphology using Gaussian mesa function modeling. *Journal of Electrocardiology*, 41(6), 588-594.
* Balestriero, R. & Baraniuk, R. G. (2018). Mad Max: Affine spline insights into deep learning. *ICML*.
* Bendale, A. & Boult, T. E. (2016). Towards open set deep networks. CVPR.
* Black, A., et al. (2022). Interpreting neural networks through the polytope lens. *AI Alignment Forum*.
* Cortes, C. & Vapnik, V. (1995). Support-vector networks. *Machine Learning*.
* Dubois, R., Maison-Blanche, P., Quenet, B., & Dreyfus, G. (2007). Automatic ECG wave extraction in long-term recordings using Gaussian mesa function models and nonlinear probability estimators. *Computer Methods and Programs in Biomedicine*.
* Erhan, D., Bengio, Y., Courville, A., & Vincent, P. (2009). Visualizing higher-layer features of a deep network. *Technical Report*, Université de Montréal.
* Extramiana, F., Dubois, R., Vaglio, M., Roussel, P., Dreyfus, G., Badilini, F., Leenhardt, A., & Maison-Blanche, P. (2010). T-wave morphology analysis with Gaussian mesa functions for drug-induced repolarization changes. *Annals of Noninvasive Electrocardiology*, 15(1), 26-35.
* Fan, Y., et al. (2023). Deep ReLU networks have surprisingly simple polytopes. *arXiv:2305.09145*.
* Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). Explaining and harnessing adversarial examples. *ICLR*.
* Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
* Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., & Stahel, W. A. (1986). *Robust Statistics: The Approach Based on Influence Functions*. Wiley.
* Hanin, B., & Rolnick, D. (2019). Deep ReLU networks have surprisingly few activation patterns. *NeurIPS*.
* Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. *Neural Computation*.
* Horn, R. A. & Johnson, C. R. (2012). *Matrix Analysis*. Cambridge University Press.
* Huber, P. J. (1964). Robust estimation of a location parameter. *Annals of Mathematical Statistics*, 35(1), 73-101.
* Ioffe, S. & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. *ICML*.
* Jolliffe, I. T. (2002). *Principal Component Analysis*, 2nd ed. Springer.
* LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. J. (2006). A tutorial on energy-based learning. In *Predicting Structured Data*.
* Ledoux, M. (2001). *The Concentration of Measure Phenomenon*. American Mathematical Society.
* Lee, J., Mammadov, M., & Ye, J. (2024). Defining neural network architecture through polytope structures of datasets. *ICML*.
* Li, X., Tegmark, M., et al. (2025). The geometry of concepts: Sparse autoencoder feature structure. *Entropy*, arXiv:2410.19750.
* Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., & Song, L. (2017). SphereFace: Deep hypersphere embedding for face recognition. *CVPR*.
* Lovász, L. & Vempala, S. (2007). The geometry of logconcave distributions and sampling algorithms. *Random Structures & Algorithms*, 30(3), 307-358.
* Mahalanobis, P. C. (1936). On the generalised distance in statistics. *Proceedings of the National Institute of Sciences of India*, 2, 49-55.
* Montúfar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. *NeurIPS*.
* Moosavi-Dezfooli, S.-M., Fawzi, A., & Frossard, P. (2016). DeepFool: A simple and accurate method to fool deep neural networks. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2574–2582.
* Nair, V. & Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. *ICML*.
* Nocedal, J. & Wright, S. J. (2006). *Numerical Optimization*. Springer.
* Olah, C., et al. (2020). Naturally occurring equivariance in neural networks. *Distill*.
* Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. *Nature*.
* Oursland, A. (2024a). "Interpreting Neural Networks through Mahalanobis Distance." *arXiv preprint arXiv:2410.19352*. 
* Oursland, A. (2024b). "Neural Networks Use Distance Metrics." *arXiv preprint arXiv:2411.17932v1*. 
* Oursland, A. (2025). "Neural Networks Learn Distance Metrics." *arXiv preprint arXiv:2502.02103*.
* Park, J., et al. (2024). The geometry of categorical and hierarchical concepts in large language models. *arXiv:2406.01506*.
* Pascanu, R., Montúfar, G. F., & Bengio, Y. (2014). On the number of response regions of deep feedforward networks with piecewise linear activations. *ICLR*.
* Rosasco, L., De Vito, E., Caponnetto, A., Piana, M., & Verri, A. (2004). Are loss functions all the same? *Neural Computation*, 16(5), 1063-1076.
* Rosch, E. (1975). Cognitive representations of semantic categories. *Journal of Experimental Psychology: General*, 104(3), 192-233.
* Serra, T., Tjandraatmadja, C., & Ramalingam, S. (2018). Bounding and counting linear regions of deep neural networks. *ICML*.
* Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2014). Intriguing properties of neural networks. *International Conference on Learning Representations (ICLR)*.
* Vapnik, V. N. (1995). *The Nature of Statistical Learning Theory*. Springer.
* Vershynin, R. (2018). *High-Dimensional Probability*. Cambridge University Press.
* Ziegler, G. M. (1995). *Lectures on Polytopes*. Springer.

