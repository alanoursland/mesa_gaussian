# **Polyhedral Mesa Gaussians: A Geometric Framework for ReLU Networks**

---

## Abstract

ReLU networks partition their input space into convex polytopes—a known mathematical fact. But what probability structure do these partitions define? We introduce the Polyhedral Mesa Gaussian (PMG): a distribution characterized by a convex polytope plateau region of maximal probability, surrounded by Gaussian falloff in aggregate constraint-violation distance. The PMG arises naturally from ReLU computation through a unified construction we call the Mesa Mahalanobis distance, which decomposes the absolute value function via ReLU and introduces plateau gaps at activation boundaries. Each ReLU neuron computes a one-sided distance from a hyperplane; the region where all neurons output zero is the plateau—the prototype region the network has learned. This framework connects neural network geometry to established work in signal processing, where Gaussian Mesa Functions have modeled ECG waveforms since 2007, and to classical statistics through the Mahalanobis distance. The PMG provides a geometric vocabulary—plateau, mesa distance, constraint violation—for precise questions about what neural networks represent.

---

## 1. Introduction

What mathematical structure does a ReLU network define over its input space?

The question is fundamental to neural network interpretability. A trained network partitions its input domain and assigns different behaviors to different regions. Understanding the geometry of this partition—and the probability structure it implies—is essential for explaining what networks learn and how they generalize.

Partial answers exist. ReLU networks are piecewise linear functions, partitioning input space into convex polytopes where the network is locally affine [Montúfar et al., 2014]. The number of linear regions can grow exponentially with depth [Arora et al., 2018], though trained networks typically converge to surprisingly simple partitions [Fan et al., 2023]. Recent work has shown that categorical concepts in large language models are represented as polytopes—often simplices—in activation space [Park et al., 2024].

These results characterize the *combinatorial* structure of ReLU networks: how many regions, what shapes, how they tile space. But they leave open the *probabilistic* interpretation. Within a region, what does activation magnitude mean? Across regions, how should we understand the transition from one to another? The standard view treats larger activations as indicating stronger feature presence—an intensity-based interpretation inherited from the earliest neural network models [McCulloch & Pitts, 1943; Rosenblatt, 1958]. Yet this interpretation lacks a rigorous mathematical foundation and conflicts with evidence that networks are sensitive to small perturbations near decision boundaries while remaining robust to large changes in activation magnitude [Oursland, 2024b].

We propose a specific answer: the **Polyhedral Mesa Gaussian** (PMG). This is a probability distribution characterized by:

1. A **plateau region** where probability is maximal and constant, shaped as a convex polytope
2. **Gaussian falloff** outside the plateau, with decay rate determined by aggregate distance from the polytope boundary

The plateau corresponds to a "prototype region"—not a single ideal input, but a set of inputs that are equally representative of a learned concept. The falloff measures deviation from this region in terms of constraint violations, connecting naturally to the Mahalanobis distance from classical statistics [Mahalanobis, 1936].

The PMG framework is not a new architecture or training procedure. It is a mathematical lens for interpreting what existing ReLU networks already compute. We show that:

- A single ReLU neuron computes a one-sided distance from a hyperplane
- A layer of ReLU neurons computes a vector of constraint violations
- Aggregating these violations yields the PMG distance function
- The region where all constraints are satisfied—where all ReLUs output zero—is the plateau

This construction has precedent. In biomedical signal processing, the Gaussian Mesa Function (GMF) has been used since 2007 to model electrocardiogram waveforms, providing flat-topped, asymmetric shapes that pure Gaussians cannot capture [Dubois et al., 2007]. Our framework generalizes the GMF from one-dimensional time series to arbitrary convex polytopes in high-dimensional space, revealing it as a special case of a broader geometric structure.

The contribution of this paper is the PMG itself: its definition, its construction via rectified distances, its connection to ReLU computation, and its relationship to prior work. We provide a mathematical vocabulary—*plateau*, *mesa distance*, *constraint violation*—for asking precise questions about neural network geometry. The interpretive implications follow from the structure: if prototypes are regions rather than points, then zero activation indicates membership rather than absence, and the standard intensity-based interpretation requires revision.

We proceed as follows. Section 2 develops the Mesa Mahalanobis distance in one dimension, showing how the piecewise GMF definition unifies into a single formula via ReLU decomposition. Section 3 extends this to the multivariate case with orthogonal constraints. Section 4 presents the full Polyhedral Mesa Gaussian with arbitrary constraint geometry. Section 5 establishes the connection to ReLU network computation. Section 6 reviews prior art and empirical support. Section 7 discusses limitations and future directions.

---


## 2. Mesa Mahalanobis Distance

The Mahalanobis distance is a fundamental measure in multivariate statistics, quantifying how far a point lies from the center of a distribution while accounting for the distribution's shape [Mahalanobis, 1936]. In one dimension, it reduces to the number of standard deviations from the mean:

$$D_M(x) = \frac{|x - \mu|}{\sigma}$$

where μ is the mean and σ is the standard deviation. This distance has a natural connection to neural network computation: a linear layer with absolute value activation computes exactly this quantity when weights and biases are appropriately parameterized [Oursland, 2024a].

We develop an extension—the Mesa Mahalanobis distance—that introduces a plateau region where the distance is zero. This construction proceeds in three steps: decomposing the absolute value via ReLU, separating the two components to create a gap, and allowing asymmetric scaling.

### 2.1 ReLU Decomposition of Absolute Value

The absolute value function can be expressed as the sum of two Rectified Linear Units [Nair & Hinton, 2010]:

$$|z| = \text{ReLU}(z) + \text{ReLU}(-z)$$

where $\text{ReLU}(z) = \max(0, z)$. This identity partitions the real line into three regions:

- When $z > 0$: $\text{ReLU}(z) = z$ and $\text{ReLU}(-z) = 0$
- When $z < 0$: $\text{ReLU}(z) = 0$ and $\text{ReLU}(-z) = -z = |z|$
- When $z = 0$: both terms vanish

The two ReLU components have disjoint support—at most one is nonzero for any input. This property will be important when we consider norms in higher dimensions.

Applying this decomposition to the Mahalanobis distance:

$$D_M(x) = \frac{\text{ReLU}(x - \mu) + \text{ReLU}(-(x - \mu))}{\sigma}$$

This reformulation is algebraically equivalent to the original, but it expresses the distance in terms of operations that neural networks compute directly.

### 2.2 Introducing the Plateau Gap

The standard Mahalanobis distance achieves its minimum (zero) at exactly one point: $x = \mu$. The mesa construction extends this minimum to an interval by shifting each ReLU activation point outward from the center.

Define the **Mesa Mahalanobis distance** with plateau half-width δ:

$$D_{MM}(x) = \frac{\text{ReLU}(x - \mu - \delta)}{\sigma} + \frac{\text{ReLU}(-x + \mu - \delta)}{\sigma}$$

This yields three regions:

**Right tail** ($x > \mu + \delta$): Only the first ReLU is active.
$$D_{MM} = \frac{x - \mu - \delta}{\sigma}$$

**Plateau** ($\mu - \delta \leq x \leq \mu + \delta$): Both ReLU arguments are non-positive, so both terms vanish.
$$D_{MM} = 0$$

**Left tail** ($x < \mu - \delta$): Only the second ReLU is active.
$$D_{MM} = \frac{\mu - \delta - x}{\sigma}$$

The parameter δ controls the half-width of the plateau; the total plateau width is 2δ. When δ = 0, the plateau collapses to a single point and we recover the standard Mahalanobis distance.

### 2.3 Asymmetric Extension

Real-world distributions are often asymmetric. The Gaussian Mesa Function developed for ECG signal processing allows independent control of ascending and descending slopes [Dubois et al., 2007]. We incorporate this by allowing different scale parameters on each side:

$$D_{MM}(x) = \frac{\text{ReLU}(x - \mu - \delta)}{\sigma_+} + \frac{\text{ReLU}(-x + \mu - \delta)}{\sigma_-}$$

where $\sigma_+$ controls the right tail decay and $\sigma_-$ controls the left tail decay. The five parameters $(A, \mu, \delta, \sigma_+, \sigma_-)$ match those of the original GMF formulation, though we use different notation to emphasize the connection to Mahalanobis distance.

### 2.4 The Mesa Gaussian

The Mesa Gaussian is obtained by applying the standard Gaussian kernel to the Mesa Mahalanobis distance:

$$G(x) = A \cdot \exp\left(-\frac{1}{2}D_{MM}^2\right)$$

Expanding by region:

$$G(x) = \begin{cases} 
A \cdot \exp\left(-\frac{(x - \mu - \delta)^2}{2\sigma_+^2}\right), & x > \mu + \delta \\[0.8em]
A, & \mu - \delta \leq x \leq \mu + \delta \\[0.8em]
A \cdot \exp\left(-\frac{(x - \mu + \delta)^2}{2\sigma_-^2}\right), & x < \mu - \delta
\end{cases}$$

This is precisely the Gaussian Mesa Function of Dubois et al. [2007], but derived from a unified formula rather than defined piecewise.

```
[FIGURE 1: Construction of the Mesa Mahalanobis distance and resulting Mesa Gaussian.

(a) Standard Mahalanobis distance D_M = |x - μ|/σ forms a V-shape with 
    minimum at x = μ. The corresponding Gaussian has a single peak.

(b) ReLU decomposition: |x - μ| = ReLU(x - μ) + ReLU(-(x - μ)). 
    Two half-lines meeting at the origin, each computed by one ReLU.

(c) Introducing the plateau gap: shifting each ReLU outward by δ creates 
    an interval [μ - δ, μ + δ] where both ReLUs output zero. 
    The distance function becomes a "tub" shape (flat bottom, linear sides).

(d) The Mesa Gaussian G(x) = A·exp(-D²_MM/2): flat plateau at maximum 
    probability A, with Gaussian tails on each side. Asymmetric version 
    shown with σ₊ ≠ σ₋.

Parameters labeled: μ (center), δ (plateau half-width), σ₊, σ₋ (tail scales), A (amplitude).]
```

### 2.5 Properties

**Continuity.** The Mesa Gaussian is continuous everywhere. At the boundary $x = \mu + \delta$, the right tail gives $\exp(0) = 1$, matching the plateau value. Similarly at $x = \mu - \delta$.

**Smoothness.** The function is infinitely differentiable everywhere except at the plateau boundaries $x = \mu \pm \delta$, where it is continuous but not differentiable. This inherits from the ReLU kinks in $D_{MM}$.

**Reduction to Gaussian.** When $\delta = 0$ and $\sigma_+ = \sigma_- = \sigma$, the Mesa Gaussian reduces to a standard Gaussian centered at μ:
$$G(x) = A \cdot \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

**Plateau interpretation.** The plateau region $[\mu - \delta, \mu + \delta]$ represents a set of equally "central" points—all at distance zero from the prototype. This contrasts with the standard Gaussian, where only the mean achieves maximal probability.

**Norm equivalence.** Because the two ReLU components have disjoint support, the L1 and L2 norms of the component vector coincide:
$$\|(a, 0)\|_1 = |a| = \|(a, 0)\|_2$$

This equivalence will extend to higher dimensions when we aggregate component distances.

### 2.6 Relation to Prior Work

The Gaussian Mesa Function was introduced for modeling electrocardiogram waveforms, where the flat-topped shape captures the sustained plateau of cardiac action potentials that pure Gaussians cannot represent [Dubois et al., 2007]. It has been applied to automatic wave extraction in long-term ECG recordings and to analysis of drug-induced cardiac repolarization changes [Badilini et al., 2008].

Our contribution is not the mesa shape itself—that is established prior art—but the unified construction via rectified distances. The piecewise definition obscures the underlying structure; the ReLU decomposition reveals it. This reformulation is what enables generalization to higher dimensions, where "left of μ" and "right of μ" have no meaning but half-space constraints do.

The connection between neural network linear layers and Mahalanobis distance was established in [Oursland, 2024a], which showed that neurons with absolute value activation can be interpreted as computing distances along principal component directions. The present work extends this by introducing the plateau gap—moving from point prototypes to region prototypes.

---

## 3. Multivariate Mesa Gaussian

Before generalizing to arbitrary polytopes, we develop the intermediate case where the plateau is a hyperrectangle aligned with principal axes. This Multivariate Mesa Gaussian (MMG) is not merely a stepping stone—it captures structure relevant to neural network computation:

**PCA-aligned representations.** When data has been whitened or when network layers perform implicit whitening—as batch normalization encourages [Ioffe & Szegedy, 2015]—the natural coordinate system is principal components. The MMG's hyperrectangle plateau aligns with this structure.

**Independent tolerances.** The hyperrectangle allows different plateau widths $\delta_i$ along each axis: independent "tolerances" for each principal component. This models situations where some dimensions require exact matching while others allow variation.

**Explicit construction.** The MMG construction via eigendecomposition makes all parameters interpretable: eigenvectors give directions, eigenvalues give scales, $\delta_i$ give tolerances. The general PMG (Section 4) trades this interpretability for flexibility.

### 3.1 Standard Multivariate Mahalanobis Distance

For a multivariate Gaussian distribution with mean $\boldsymbol{\mu} \in \mathbb{R}^n$ and covariance matrix $\Sigma \in \mathbb{R}^{n \times n}$, the Mahalanobis distance is [Mahalanobis, 1936]:

$$D_M(\mathbf{x}) = \sqrt{(\mathbf{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})}$$

Through eigendecomposition $\Sigma = V \Lambda V^\top$, where $V$ contains orthonormal eigenvectors and $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$ contains eigenvalues, this distance decomposes along principal axes [Jolliffe, 2002]. Defining the projection onto the $i$-th principal axis as $z_i = \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu})$:

$$D_M(\mathbf{x}) = \sqrt{\sum_{i=1}^{n} \frac{z_i^2}{\lambda_i}} = \left\| \left( \frac{|z_1|}{\sqrt{\lambda_1}}, \ldots, \frac{|z_n|}{\sqrt{\lambda_n}} \right) \right\|_2$$

The multivariate Mahalanobis distance is thus the L2 norm of univariate Mahalanobis distances along each principal axis.

### 3.2 Introducing Plateau Gaps

Applying the ReLU decomposition from Section 2 to each axis, and shifting activation points outward by gap parameters $\delta_i$, we obtain the half-component distances:

$$D_{M,i}^+ = \frac{\text{ReLU}(z_i - \delta_i)}{\sqrt{\lambda_i}}, \quad D_{M,i}^- = \frac{\text{ReLU}(-z_i - \delta_i)}{\sqrt{\lambda_i}}$$

Along each axis $i$, this creates three regions:

- **Positive tail** ($z_i > \delta_i$): $D_{M,i}^+ > 0$, $D_{M,i}^- = 0$
- **Plateau** ($-\delta_i \leq z_i \leq \delta_i$): $D_{M,i}^+ = 0$, $D_{M,i}^- = 0$
- **Negative tail** ($z_i < -\delta_i$): $D_{M,i}^+ = 0$, $D_{M,i}^- > 0$

The **Multivariate Mesa Mahalanobis distance** aggregates these via the L2 norm:

$$D_{MM}(\mathbf{x}) = \left\| (D_{M,1}^+, D_{M,1}^-, \ldots, D_{M,n}^+, D_{M,n}^-) \right\|_2$$

Since at most one of $D_{M,i}^+$ or $D_{M,i}^-$ is nonzero for each axis (they have disjoint support), at most $n$ of the $2n$ terms contribute for any input.

### 3.3 Plateau Geometry

The plateau region—where $D_{MM} = 0$—occurs when all half-components vanish simultaneously. This requires $|z_i| \leq \delta_i$ for all $i$:

$$\mathcal{P} = \left\{ \mathbf{x} : |z_i| \leq \delta_i \text{ for all } i = 1, \ldots, n \right\}$$

In principal component coordinates, this is a **hyperrectangle** centered at the origin with half-widths $\delta_1, \delta_2, \ldots, \delta_n$. In the original coordinate system, the hyperrectangle is rotated by $V$, yielding an oriented box centered at $\boldsymbol{\mu}$. The faces of this box are aligned with the eigenvectors of the covariance matrix—the directions of maximal and minimal variance in the data.

```
[FIGURE 2: Multivariate Mesa Gaussian plateau geometry. 
A 2D example showing the hyperrectangle plateau (shaded) aligned with 
principal axes (dashed), rotated relative to original coordinates. 
Iso-contours at D_MM = 1, 2, 3 shown as rounded rectangles 
transitioning toward ellipses far from plateau.]
```

### 3.4 The Multivariate Mesa Gaussian

The **Multivariate Mesa Gaussian** (MMG) applies the Gaussian kernel to the Mesa Mahalanobis distance:

$$G(\mathbf{x}) = A \cdot \exp\left(-\frac{1}{2} D_{MM}^2(\mathbf{x})\right)$$

Inside the plateau, $G(\mathbf{x}) = A$. Outside, probability decays as a Gaussian in aggregate constraint violation.

For full generality, independent scale parameters $\lambda_i^+$ and $\lambda_i^-$ can control decay rates in positive and negative directions along each axis, mirroring the asymmetric univariate case. The MMG then has $\frac{n(n-1)}{2} + 4n + 1$ parameters: $n(n-1)/2$ for the orthogonal matrix $V$, $n$ for the center $\boldsymbol{\mu}$, $n$ for plateau widths $\delta_i$, $2n$ for asymmetric variances, and 1 for amplitude. This adds $2n$ parameters beyond a standard multivariate Gaussian (the plateau widths and asymmetry).

### 3.5 Reduction to Special Cases

| Condition | Result |
|-----------|--------|
| $n = 1$ | Univariate Mesa Gaussian (GMF) |
| $\delta_i = 0$ for all $i$ | Standard multivariate Gaussian |
| $\lambda_i^+ = \lambda_i^-$ for all $i$ | Symmetric MMG |
| $\Sigma = \sigma^2 I$, equal $\delta_i$ | Isotropic MMG with hypercube plateau |

The hierarchy is clear: the univariate construction lifts to multiple dimensions by applying the mesa transformation independently along each principal axis, with the L2 norm aggregating the results. The constraint to orthogonal, antipodal pairs of faces (inherited from PCA) is what Section 4 will relax.

---

## 4. Polyhedral Mesa Gaussian

The Multivariate Mesa Gaussian constrains the plateau to be a hyperrectangle aligned with orthogonal principal axes. This reflects the structure of PCA-based covariance decomposition but limits expressiveness. Many natural boundaries—decision regions in classification, constraint sets in optimization, concept boundaries in representation spaces—are polytopes with faces at arbitrary angles.

The Polyhedral Mesa Gaussian (PMG) relaxes the orthogonality requirement. The plateau becomes an arbitrary convex polytope defined by the intersection of half-spaces, and the falloff measures aggregate violation of these constraints. This generalization connects directly to ReLU network computation, where each neuron defines a half-space and layers compose constraints without orthogonality restrictions.

### 4.1 From Orthogonal Axes to Arbitrary Hyperplanes

In the MMG, each principal axis contributes two constraints: one for exceeding the upper plateau boundary, one for falling below the lower boundary. These constraints come in antipodal pairs (opposite-facing normals) and are mutually orthogonal across axes.

The PMG replaces this structure with $m$ arbitrary half-space constraints. Each constraint $j$ is defined by:

| Parameter | Symbol | Description |
|-----------|--------|-------------|
| Normal vector | $\mathbf{n}_j \in \mathbb{R}^n$ | Outward-pointing direction |
| Offset | $b_j \in \mathbb{R}$ | Signed distance from origin to boundary |
| Scale | $\sqrt{\lambda_j} > 0$ | Controls decay rate in tail |

The signed distance from a point $\mathbf{x}$ to the $j$-th hyperplane boundary is:

$$d_j(\mathbf{x}) = \mathbf{n}_j^\top \mathbf{x} - b_j$$

with the convention that $d_j > 0$ means the point lies outside (in the direction of the outward normal) and $d_j < 0$ means inside.

### 4.2 Component Distance with Plateau Margin

Each constraint contributes a one-sided distance that activates only when the point exceeds the plateau boundary:

$$D_j(\mathbf{x}) = \frac{1}{\sqrt{\lambda_j}} \text{ReLU}(d_j(\mathbf{x})) = \frac{1}{\sqrt{\lambda_j}} \text{ReLU}(\mathbf{n}_j^\top \mathbf{x} - b_j)$$

This construction directly parallels the half-components in the MMG. The ReLU ensures that:

- Points satisfying the constraint ($d_j \leq 0$) contribute zero to the distance
- Points violating the constraint ($d_j > 0$) contribute proportionally to their violation, scaled by $1/\sqrt{\lambda_j}$

A separate plateau margin parameter $\delta_j$ can be incorporated by adjusting the offset: replacing $b_j$ with $b_j + \delta_j$ shifts the activation boundary outward from the polytope face. For notational simplicity, we absorb this into $b_j$.

### 4.3 Plateau Region

The plateau is the set of points satisfying all constraints:

$$\mathcal{P} = \left\{ \mathbf{x} \in \mathbb{R}^n : d_j(\mathbf{x}) \leq 0 \text{ for all } j = 1, \ldots, m \right\}$$

This is the intersection of $m$ half-spaces—a **convex polytope** by definition [Ziegler, 1995]. The polytope may be:

- **Bounded**: A finite region enclosed by the constraints (e.g., a simplex, cube, or more complex shape)
- **Unbounded**: Extending to infinity in some directions if constraints don't fully enclose the region
- **Empty**: If the constraints are mutually inconsistent

For the PMG to define a proper probability distribution, the plateau should be bounded. Unbounded plateaus yield improper distributions (infinite integral) unless additional bounding constraints are added.

```
[Figure 3: Plateau regions and iso-distance contours for different Mesa Gaussian variants.

(a) Multivariate Mesa Gaussian (MMG): The plateau is a rectangle aligned 
    with principal axes (shown rotated in original coordinates). 
    Four constraints (two antipodal pairs along orthogonal axes).
    Iso-contours shown at D_MM = 1, 2, 3—rounded rectangles transitioning 
    toward ellipses far from plateau.

(b) Polyhedral Mesa Gaussian (PMG) with triangular plateau: Three 
    non-orthogonal constraints forming a simplex. Iso-contours show 
    smooth falloff from triangular plateau toward rounded triangular 
    shapes at larger distances.

(c) PMG with pentagonal plateau: Five constraints at arbitrary angles.
    Demonstrates that any convex polygon (2D) or polytope (nD) can 
    serve as plateau region.

(d) Limiting cases: As δ → 0, plateau collapses to a point (standard 
    Gaussian). As all λ_j → 0, iso-contours sharpen toward hard 
    polytope boundary.

Shading indicates probability density (darker = higher).]
```

### 4.4 Polyhedral Mesa Distance

The **Polyhedral Mesa distance** aggregates constraint violations via the L2 norm:

$$D_{PM}(\mathbf{x}) = \left\| (D_1(\mathbf{x}), D_2(\mathbf{x}), \ldots, D_m(\mathbf{x})) \right\|_2 = \sqrt{\sum_{j=1}^{m} D_j(\mathbf{x})^2}$$

This measures how far a point lies from the plateau in terms of aggregate constraint violation. Points inside the plateau have $D_{PM} = 0$. Points outside have $D_{PM} > 0$, with contributions from each violated constraint.

### 4.5 The Polyhedral Mesa Gaussian

The **Polyhedral Mesa Gaussian** (PMG) is:

$$G(\mathbf{x}) = A \cdot \exp\left(-\frac{1}{2} D_{PM}(\mathbf{x})^2\right)$$

Equivalently:

$$G(\mathbf{x}) = A \cdot \exp\left(-\frac{1}{2} \sum_{j=1}^{m} \frac{\text{ReLU}(\mathbf{n}_j^\top \mathbf{x} - b_j)^2}{\lambda_j}\right)$$

The normalization constant $A$ ensures the distribution integrates to one. Unlike the standard Gaussian, $A$ generally has no closed-form expression and requires numerical integration over the polytope and its exterior regions.

### 4.6 Properties

**Convexity of plateau.** The plateau $\mathcal{P}$ is always convex, being the intersection of half-spaces. This is a fundamental constraint of the PMG framework—it cannot represent non-convex prototype regions with a single component.

**Convexity of negative log-likelihood.** The energy function:

$$E(\mathbf{x}) = -\log G(\mathbf{x}) = \frac{1}{2} D_{PM}(\mathbf{x})^2 + \text{const}$$

is convex. It is a sum of squared ReLU functions, each of which is convex, and sums of convex functions are convex [Boyd & Vandenberghe, 2004]. This ensures that maximum likelihood estimation over the PMG has no local minima—a favorable property for optimization.

**Continuity and smoothness.** The PMG is continuous everywhere. It is smooth ($C^\infty$) except at the polytope faces, where the ReLU kinks introduce gradient discontinuities. At edges and vertices where multiple faces meet, multiple ReLU components transition simultaneously.

**Asymptotic behavior.** Far from the plateau, all constraints are violated and the decay is dominated by the constraint with largest $\lambda_j$ (slowest decay). Asymptotically, the PMG resembles a Gaussian aligned with the softest constraint direction.

### 4.7 Relationship to Hinge Loss

The component distance $D_j = \frac{1}{\sqrt{\lambda_j}} \text{ReLU}(\mathbf{n}_j^\top \mathbf{x} - b_j)$ has the same functional form as the hinge loss from support vector machines [Cortes & Vapnik, 1995]:

$$L_{\text{hinge}} = \text{ReLU}(\text{margin} - \text{signed distance})$$

The PMG uses the opposite sign convention—penalizing points *outside* the margin rather than inside—but the underlying structure is identical. Squared hinge loss $\text{ReLU}(\cdot)^2$ is a common SVM variant and corresponds directly to our $D_j^2$ terms.

This connection frames the PMG as a probabilistic relaxation of hard polytope constraints: instead of binary membership (in or out), we have graded probability that decays smoothly with constraint violation.

### 4.8 Norm Considerations

The L2 norm is not the only choice for aggregating component distances. Alternatives include:

**L1 norm:** $D_{PM}^{(L1)} = \sum_j D_j$

This corresponds more directly to what a linear layer computes (weighted sums) and produces diamond-shaped iso-contours. However, it counts correlated constraint violations additively, potentially over-penalizing points near polytope edges where multiple similar constraints are violated.

**L∞ norm:** $D_{PM}^{(L\infty)} = \max_j D_j$

This measures distance by the single most-violated constraint, producing box-shaped iso-contours. It avoids double-counting correlated violations but introduces non-smoothness where the maximizing constraint changes.

The L2 norm offers a balance: smooth everywhere except at ReLU kinks, and it reduces to the standard Mahalanobis distance in appropriate limits.

#### Section 4.8.1 — High-Dimensional Behavior

The relationship between L1 and L2 norms has different character in high dimensions than intuition from 2D or 3D suggests.

**Worst-case divergence.** For a vector with $k$ nonzero components of equal magnitude $a$, the norms are $\|d\|_1 = ka$ and $\|d\|_2 = \sqrt{k}a$, so $\|d\|_1 / \|d\|_2 = \sqrt{k}$. In the worst case where all $m$ constraints are violated equally, the L1 norm exceeds L2 by factor $\sqrt{m}$.

**Sparsity saves the correspondence.** However, the PMG framework predicts sparse constraint violations. Inputs inside or near the plateau violate few constraints; only far-outliers violate many. When $k \ll m$ constraints are violated:

$$\|d\|_2 \leq \|d\|_1 \leq \sqrt{k} \|d\|_2$$

For typical inputs where $k = O(1)$ regardless of ambient dimension, the norms are within a small constant factor.

**Near-orthogonality in high dimensions.** Random unit vectors in $\mathbb{R}^n$ are nearly orthogonal with high probability: for two independent random unit vectors, $|\mathbf{n}_i^\top \mathbf{n}_j| = O(1/\sqrt{n})$ [Vershynin, 2018]. If constraint normals are approximately random (as might occur in learned networks without explicit orthogonality pressure), the Gram matrix $G$ is close to identity, and the decorrelation correction (Section 4.9) becomes negligible.

This suggests the L2 PMG is most accurate when:
- Few constraints are violated (sparse activation)
- Constraint normals are approximately orthogonal (random or decorrelated)

Both conditions are plausible for well-trained networks operating on in-distribution data.


### 4.9 Non-Orthogonal Constraints

When constraint normals are not orthogonal, their violations are correlated. A point outside a polytope corner violates multiple constraints that partially measure the same deviation. The L2 norm counts these as independent contributions, which can over-penalize corners relative to face centers.

For unit normals, the Gram matrix $G_{ij} = \mathbf{n}_i^\top \mathbf{n}_j$ captures angular relationships. A decorrelated distance:

$$D_{\text{corrected}}^2 = \mathbf{D}^\top G^{-1} \mathbf{D}$$

accounts for constraint correlations and reduces to the L2 norm when normals are orthogonal ($G = I$). This correction can be singular when normals are linearly dependent and may require regularization in practice.

For the present treatment, we use the uncorrected L2 norm, accepting that the geometry at corners differs from the Euclidean distance to the polytope boundary.

### 4.10 Reduction to Special Cases

| Condition | Result |
|-----------|--------|
| $m = 2n$ constraints as antipodal orthogonal pairs | Multivariate Mesa Gaussian |
| $m = 2$, $n = 1$ (two opposite-facing constraints) | Univariate Mesa Gaussian (GMF) |
| All $\delta_j = 0$, orthogonal unit normals | Standard multivariate Gaussian |
| All $\lambda_j \to 0$ | Hard indicator function on polytope |
| All $\lambda_j \to \infty$ | Constant function (uniform over $\mathbb{R}^n$) |

The PMG thus sits at the apex of a hierarchy: it generalizes the MMG by relaxing orthogonality, generalizes the GMF by extending to multiple dimensions and arbitrary polytopes, and generalizes the standard Gaussian by introducing plateau regions.

```
[FIGURE 4: Hierarchy of Mesa Gaussian distributions.

Diagram showing specialization relationships:

                    PMG
                 (arbitrary convex polytope)
                    ↓
         ┌─────────┴─────────┐
         ↓                   ↓
       MMG                Simplex MG
   (hyperrectangle,      (n+1 faces,
    2n constraints,       minimal enclosure)
    orthogonal pairs)
         ↓
       GMF
   (interval plateau,
    2 constraints,
    1D)
         ↓
    Standard Gaussian
   (point plateau, δ = 0)

Each level shows characteristic plateau shape and constraint count.]
```

### 4.11 Parameter Summary

| Parameter | Count | Description |
|-----------|-------|-------------|
| $\mathbf{n}_j$ | $m \times n$ | Constraint normal vectors |
| $b_j$ | $m$ | Constraint offsets |
| $\lambda_j$ | $m$ | Tail decay scales |
| $A$ | 1 | Normalization (determined by other parameters) |

**Total**: $m(n + 2)$ parameters (or $m(n + 1) + 1$ if normals are constrained to unit length).

The parameter count scales with the number of constraints $m$ rather than the ambient dimension $n$. A simple polytope (e.g., a simplex with $n + 1$ faces) requires $O(n^2)$ parameters, while a complex polytope with many faces requires correspondingly more.

---


## 5. Connection to ReLU Networks

The Polyhedral Mesa Gaussian is not merely analogous to ReLU network computation—it formalizes exactly what ReLU layers compute. Each neuron defines a half-space constraint; each layer assembles these into a polytope; the activation pattern encodes which constraints are satisfied versus violated. This section makes the correspondence precise.

### 5.1 Single ReLU Neuron as Half-Space Distance

A single ReLU neuron computes:

$$h = \text{ReLU}(\mathbf{w}^\top \mathbf{x} + b)$$

where $\mathbf{w} \in \mathbb{R}^n$ is the weight vector and $b \in \mathbb{R}$ is the bias. This is algebraically identical to the PMG component distance:

$$D_j(\mathbf{x}) = \frac{1}{\sqrt{\lambda_j}} \text{ReLU}(\mathbf{n}_j^\top \mathbf{x} - b_j)$$

under the identification:

| Neural Network | PMG |
|----------------|-----|
| Weight vector $\mathbf{w}$ | Scaled normal $\mathbf{n}_j / \sqrt{\lambda_j}$ |
| Bias $b$ | Scaled offset $-b_j / \sqrt{\lambda_j}$ |
| Weight magnitude $\|\mathbf{w}\|$ | Inverse scale $1/\sqrt{\lambda_j}$ |

Decomposing the weight vector into magnitude and direction:

$$\mathbf{w} = \|\mathbf{w}\| \cdot \hat{\mathbf{w}}, \quad \hat{\mathbf{w}} = \frac{\mathbf{w}}{\|\mathbf{w}\|}$$

the neuron computes:

$$h = \text{ReLU}\left(\|\mathbf{w}\| \cdot \hat{\mathbf{w}}^\top \mathbf{x} + b\right) = \|\mathbf{w}\| \cdot \text{ReLU}\left(\hat{\mathbf{w}}^\top \mathbf{x} + \frac{b}{\|\mathbf{w}\|}\right)$$

This is the one-sided distance from the hyperplane $\hat{\mathbf{w}}^\top \mathbf{x} = -b/\|\mathbf{w}\|$, scaled by the weight magnitude. The unit vector $\hat{\mathbf{w}}$ defines the hyperplane normal; the ratio $-b/\|\mathbf{w}\|$ determines the hyperplane's position along that direction; and $\|\mathbf{w}\|$ controls the sensitivity to deviations—larger weights mean faster activation growth with distance.

The neuron outputs zero when $\mathbf{x}$ lies on the "inside" of the hyperplane (the half-space where $\mathbf{w}^\top \mathbf{x} + b \leq 0$) and outputs a positive value proportional to distance when $\mathbf{x}$ lies outside. This is precisely a PMG component distance [Oursland, 2024a].

### 5.2 ReLU Layer as Constraint Violation Vector

A layer of $m$ ReLU neurons computes:

$$\mathbf{h} = \text{ReLU}(W\mathbf{x} + \mathbf{b})$$

where $W \in \mathbb{R}^{m \times n}$ is the weight matrix (each row $\mathbf{w}_j$ defines one neuron) and $\mathbf{b} \in \mathbb{R}^m$ is the bias vector. The output $\mathbf{h} \in \mathbb{R}^m_{\geq 0}$ is a vector of non-negative activations.

Under the PMG interpretation, this is a vector of constraint violation distances:

$$\mathbf{h} = (D_1(\mathbf{x}), D_2(\mathbf{x}), \ldots, D_m(\mathbf{x}))$$

Each component $h_j$ measures how far $\mathbf{x}$ lies outside the $j$-th half-space constraint. The ReLU ensures that satisfied constraints contribute zero.

The $m$ neurons collectively define $m$ hyperplanes in $\mathbb{R}^n$. The intersection of their "inside" half-spaces is a convex polytope:

$$\mathcal{P} = \{\mathbf{x} : W\mathbf{x} + \mathbf{b} \leq \mathbf{0}\}$$

This is precisely the PMG plateau region. Points inside $\mathcal{P}$ produce the all-zeros activation pattern $\mathbf{h} = \mathbf{0}$; points outside produce positive activations indicating which constraints are violated and by how much.

```
[FIGURE 5: Correspondence between ReLU network computation and PMG geometry.

(a) Single neuron: A ReLU neuron with weights w and bias b defines a 
    hyperplane (shown as line in 2D). The "inside" half-space (where 
    w^T x + b ≤ 0) produces zero activation. Points outside activate 
    proportionally to their distance from the boundary. Arrow shows 
    weight direction (outward normal).

(b) Layer of neurons: Three neurons define three hyperplanes. Their 
    intersection (shaded region) is the plateau—the set of inputs 
    producing the all-zeros activation pattern h = (0, 0, 0).

(c) Activation pattern diagram: For a point outside the plateau, 
    show which neurons activate (violated constraints) and their 
    activation magnitudes (distances). Point near a single face 
    activates one neuron strongly; point near a vertex activates 
    multiple neurons.

(d) Interpretation comparison: 
    Standard view: "Neuron 1 detects feature F₁ with strength h₁"
    PMG view: "Input violates constraint 1 by distance h₁"
    The plateau (h = 0) is the prototype region, not a dead zone.]
```

### 5.3 Layer Composition and Distance Aggregation

Consider a two-layer network:

$$\mathbf{h} = \text{ReLU}(W_1 \mathbf{x} + \mathbf{b}_1)$$
$$\mathbf{y} = W_2 \mathbf{h} + \mathbf{b}_2$$

The first layer computes the constraint violation vector. The second layer computes linear combinations of these violations. When $W_2$ has non-negative entries, each output is a weighted sum of distances:

$$y_k = \sum_{j} w_{kj}^{(2)} D_j(\mathbf{x}) + b_k^{(2)}$$

This is an L1-style aggregation of constraint violations—structurally similar to the PMG distance, though using weighted L1 rather than L2. Each output $y_k$ can be interpreted as a Mesa distance for a different polytope:

- Non-zero weights $w_{kj}^{(2)}$ select which constraints participate in the $k$-th polytope
- The weight magnitudes adjust relative importance (effective $\lambda_j$ values)
- Zero weights exclude constraints entirely
- The bias $b_k^{(2)}$ shifts the overall threshold

Different outputs can share constraints (same first-layer neurons) while weighting them differently—a form of parameter efficiency where $m$ hyperplanes define a combinatorial family of possible polytopes.

### 5.4 The Plateau as Prototype Region

The PMG framework identifies the plateau—the region where $D_{PM} = 0$—as the **prototype region**. For a ReLU network, this corresponds to the activation pattern where all relevant neurons output zero.

This inverts the standard interpretation of neural network activations [McCulloch & Pitts, 1943; Rosenblatt, 1958]:

| Standard Interpretation | PMG Interpretation |
|------------------------|---------------------|
| High activation = feature detected | High activation = constraint violated |
| Neuron "fires" when input matches | Neuron "fires" when input deviates |
| Prototype maximizes activation | Prototype yields zero activation |
| Zero activation = feature absent | Zero activation = inside prototype region |

Under the PMG view, the "ideal" inputs for a learned representation are not those that maximize neuron activations, but those that *minimize* them—the inputs lying within the plateau region where all constraints are satisfied.

This reframing aligns with empirical evidence that trained networks are sensitive to small perturbations near decision boundaries while remaining robust to changes in activation magnitude [Oursland, 2024b]. If networks used intensity (larger = better), scaling activations should matter; if they use distance (smaller = closer to prototype), boundary position should matter. Experiments support the latter [Oursland, 2025].

### 5.5 Sparsity Reinterpreted

Sparse activations—where most neurons output zero—are often considered desirable in neural networks [Glorot et al., 2011]. The standard interpretation frames sparsity as "most features absent." The PMG interpretation reframes it as "most constraints satisfied."

An input producing sparse activations lies inside most constraint half-spaces, violating only a few. This is the expected behavior for well-formed inputs: they should satisfy most of the learned constraints, deviating only along a small number of dimensions.

Inputs that activate many neurons are those that violate many constraints—potentially anomalous or out-of-distribution points. This perspective connects naturally to anomaly detection, where distance from a learned prototype region indicates abnormality [Ruff et al., 2018].

### 5.6 Depth and Hierarchical Constraints

Deeper networks compose multiple ReLU layers:

$$\mathbf{h}_1 = \text{ReLU}(W_1 \mathbf{x} + \mathbf{b}_1)$$
$$\mathbf{h}_2 = \text{ReLU}(W_2 \mathbf{h}_1 + \mathbf{b}_2)$$
$$\vdots$$

Each layer defines constraints on the representation space of the previous layer. The first layer partitions input space into polytopes; the second layer partitions *distance space*; subsequent layers refine further.

A key observation: the input to layer $\ell + 1$ is a vector of non-negative values (constraint violations from layer $\ell$). The "origin" of this space—the point $\mathbf{h}_\ell = \mathbf{0}$—represents inputs that satisfied all previous constraints. Subsequent layers define constraints relative to this origin.

This hierarchical structure allows complex, non-convex regions in input space to be represented through compositions of convex constraints in successive representation spaces. The preimage of a convex polytope in $\mathbf{h}_L$ space can be a union of convex regions in input space—explaining how deep networks achieve non-convex decision boundaries while each layer operates with convex primitives [Montúfar et al., 2014].

### 5.7 Negative Weights and the Limits of the Correspondence

The PMG-ReLU correspondence is cleanest for non-negative weight combinations, which implement constraint intersection (AND logic). When all weights in the aggregation layer are non-negative, each output computes a weighted sum of constraint violations—a soft measure of "how far outside the polytope." Negative weights introduce richer operations that the basic PMG framework does not directly capture.

**What negative weights enable.** Negative weights allow computations beyond constraint intersection:

*Relative comparison.* An output $y = w_1 h_1 - w_2 h_2$ with $w_1 > 0$ and $w_2 > 0$ compares violations of two constraints. The output is positive when constraint 1 is violated more than constraint 2—a "this more than that" computation rather than "this and that."

*Complement-like operations.* A negative weight on constraint $j$ means that *satisfying* $j$ (small $h_j$) contributes positively to the output after aggregation. This locally inverts the PMG logic: satisfaction becomes evidence rather than violation.

*Non-convex regions.* Through differences of constraint violations, negative weights can carve out non-convex regions. A network might learn "inside polytope A but outside polytope B"—the set-theoretic difference $A \setminus B$, which is generally non-convex even when $A$ and $B$ are convex.

**Connection to DC programming.** These operations fit naturally into the **difference of convex (DC) functions** framework [Tao & An, 1997]. Any function expressible as $f(x) = g(x) - h(x)$ where $g$ and $h$ are both convex is a DC function. A ReLU layer followed by a linear layer with mixed-sign weights computes exactly this: separate the positive and negative weights, and the output is a difference of two convex aggregations of constraint violations.

DC functions are well-studied, with optimization algorithms and representation theorems. The relevant result here is that DC functions can represent any continuous function on a compact domain [Hartman, 1959]—so the expressiveness concern is not whether mixed-weight networks *can* represent complex regions, but how to interpret what they represent geometrically.

The natural extension is a **DC-PMG**: a difference of two Polyhedral Mesa Gaussians. The "prototype region" becomes $\mathcal{P}_+ \setminus \mathcal{P}_-$, the set of points inside the positive-weight polytope but outside the negative-weight polytope. This can be non-convex, multiply connected, or even empty. The geometric interpretation becomes less intuitive: instead of "how far outside the prototype," we have "how much more outside $\mathcal{P}_-$ than $\mathcal{P}_+$."

**Scope of the present treatment.** A complete geometric account of negative weights—characterizing what region structures DC-PMGs can represent, how depth interacts with DC decomposition, and whether trained networks learn interpretable DC structure—remains open. We note that the positive-weight regime, where the PMG interpretation is clean, may capture the "backbone" of network computation: the primary polytope regions that define category prototypes. Negative weights may then provide refinements—carving exceptions, implementing relative judgments, and enabling the non-convex boundaries that complex categories require.

**The PMG as vocabulary, not claim.** More broadly, trained networks may not learn PMG structures exactly. The claim is not that every ReLU network implements a PMG, but that the PMG provides a well-founded geometric vocabulary for interpreting what ReLU networks compute. The vocabulary includes terms for what positive-weight aggregation computes (constraint intersection, polytope plateaus) and pointers to what negative weights add (DC structure, set differences). Whether this vocabulary reveals useful structure in any particular network is an empirical matter—the PMG is a lens that may or may not illuminate what a given network has learned.

### 5.8 Summary

The correspondence between PMG and ReLU networks can be summarized:

| PMG Component | ReLU Network Equivalent |
|---------------|------------------------|
| Half-space constraint | Single neuron |
| Constraint normal $\mathbf{n}_j$ | Weight direction $\hat{\mathbf{w}}_j$ |
| Scale $1/\sqrt{\lambda_j}$ | Weight magnitude $\|\mathbf{w}_j\|$ |
| Plateau boundary $b_j$ | Bias $-b_j$ |
| Component distance $D_j$ | Neuron activation $h_j$ |
| Plateau region $\mathcal{P}$ | All-zeros activation pattern |
| PMG distance $D_{PM}$ | Aggregated activation (next layer) |

The PMG is not a new architecture—it is a mathematical characterization of the geometry that standard ReLU networks already define.

---


## 6. Prior Art and Validation

The Polyhedral Mesa Gaussian is not constructed in isolation. It connects three independent lines of research: signal processing, where mesa-shaped functions have been used for decades; neural network theory, where polytope structures are empirically confirmed; and multivariate statistics, where the Mahalanobis distance provides foundational methodology. This convergence from distinct fields suggests the PMG captures something fundamental rather than arbitrary.

### 6.1 Gaussian Mesa Functions in Signal Processing

The mesa shape emerged from practical necessity in biomedical signal processing. Dubois et al. [2007] introduced the Gaussian Mesa Function for automatic extraction of electrocardiogram (ECG) waveforms. The problem: ECG signals contain distinct waves (P, QRS, T) that must be identified and characterized for clinical analysis. Standard Gaussian functions fail because cardiac waveforms are often asymmetric and flat-topped—the plateau of the ST segment, for instance, reflects sustained cardiac depolarization that a peaked Gaussian cannot capture.

The GMF solution provides five parameters per wave: amplitude, temporal position, ascending width, descending width, and plateau duration. This parameterization directly models the physiological structure of cardiac signals. The approach was validated on standard databases (MIT-BIH Arrhythmia Database, AHA Database) and has been applied to long-term ECG monitoring [Dubois et al., 2007], drug-induced cardiac effects [Badilini et al., 2008], and repolarization morphology tracking [Extramiana et al., 2010].

The GMF was defined piecewise—separate equations for left tail, plateau, and right tail. Our contribution is recognizing that this piecewise structure unifies into a single formula via ReLU decomposition, and that this formula generalizes naturally to higher dimensions. The mesa shape was not invented to explain neural networks; it arose independently from the demands of modeling biological signals. That the same geometry appears in both contexts is evidence of underlying structure.

### 6.2 Polytope Structure in Neural Networks

ReLU networks partition their input space into convex polytopes—this is a mathematical fact following from the piecewise linearity of ReLU [Montúfar et al., 2014]. Within each polytope (linear region), the network computes an affine function; at polytope boundaries, the function changes slope. The theoretical capacity for linear regions grows exponentially with depth, enabling deep networks to represent highly complex partitions [Arora et al., 2018].

Recent empirical work has examined what polytope structures trained networks actually learn:

**Simple polytopes.** Fan et al. [2023] found that despite theoretical capacity for complex partitions, trained networks converge to "surprisingly simple" polytopes. This implicit bias toward simplicity aligns with the PMG framework: if networks learn prototype regions, these regions should match the natural structure of data clusters, which tend to be compact and simply shaped rather than geometrically complex.

**Categorical concepts as polytopes.** Park et al. [2024] analyzed large language models (Gemma-2B, LLaMA-3-8B) and found that categorical concepts are represented as polytopes—specifically, simplices. A simplex is the minimal polytope enclosing a region in $n$ dimensions ($n+1$ faces). Hierarchically related concepts are orthogonal; complex categories are direct sums of simplicial components. This finding directly supports the PMG interpretation: concepts correspond to bounded polytope regions, not single points.

**Polytope-based classification.** Lee et al. [2024] showed that standard image datasets (MNIST, Fashion-MNIST, CIFAR-10) can be efficiently covered by small numbers of polytopes with few faces. They derive that network width should scale with the number of polytope faces needed to bound class regions—a direct connection between PMG geometry and architectural requirements.

**Feature geometry.** Li et al. [2025] examined sparse autoencoder features in language models and found geometric "crystal" structures with regular polyhedral relationships between concept representations. The atomic-scale structure shows parallelogram and trapezoid faces—consistent with concepts embedded in a polytope-like arrangement.

These findings were not designed to test the PMG framework, but they consistently support its geometric predictions: trained networks learn polytope-bounded regions, these regions are simpler than theoretical capacity allows, and semantic categories correspond to polytope structure in representation space.

### 6.3 Mahalanobis Distance and Statistical Foundations

The Mahalanobis distance [Mahalanobis, 1936] is a cornerstone of multivariate statistics, providing a scale-invariant measure of distance from a distribution center that accounts for correlations between dimensions. It underlies discriminant analysis [McLachlan, 2019], outlier detection [De Maesschalck et al., 2000], and Gaussian mixture models [Reynolds, 2009].

The PMG construction is a direct extension of the Mahalanobis distance:

1. **Standard Mahalanobis**: Distance from a point (the mean)
2. **Mesa Mahalanobis**: Distance from a region (the plateau)
3. **Polyhedral Mesa**: Distance from an arbitrary convex polytope

Each step generalizes the previous while preserving the core structure: project onto relevant directions, measure deviation, aggregate via norm. The connection to PCA eigendecomposition (Section 3) shows that the MMG is the natural "plateau extension" of standard Gaussian geometry.

Previous work established that neural network linear layers with absolute value activation compute Mahalanobis distances along principal component directions [Oursland, 2024a]. Empirical studies confirmed that trained networks are sensitive to distance-based perturbations while robust to intensity-based perturbations [Oursland, 2024b], and that architectures explicitly designed for distance computation outperform those constrained to intensity representations [Oursland, 2025]. The PMG framework extends this line of research by introducing the plateau—moving from point prototypes to region prototypes.

### 6.4 Connections to Related Frameworks

Several other frameworks share conceptual territory with the PMG:

**Radial Basis Function networks** [Broomhead & Lowe, 1988] explicitly use distance from learned centers, but with spherical (isotropic) Gaussian kernels rather than polytope plateaus. RBF networks struggled to scale to high dimensions partly because point prototypes require exponentially many centers to cover space—a limitation that region prototypes may address.

**Energy-based models** [LeCun et al., 2006] interpret neural network outputs as energies, with low energy indicating high probability. The PMG fits naturally here: $D_{PM}^2/2$ is the energy, and the Gaussian kernel converts energy to (unnormalized) probability. The plateau is the zero-energy region.

**Prototype networks** [Snell et al., 2017] learn point embeddings for few-shot classification, with distance to prototypes determining class membership. The PMG generalizes this from point prototypes to region prototypes, potentially offering greater robustness.

**Metric learning** [Weinberger & Saul, 2009] explicitly trains networks to minimize intra-class distances and maximize inter-class distances—precisely the structure PMG predicts networks should learn implicitly.

### 6.5 What the PMG Provides

The PMG framework does not introduce new empirical findings. Its contribution is organizational: it provides a single mathematical object that connects:

- A signal processing technique (GMF) used for two decades
- Empirically observed polytope structure in trained networks
- Classical statistical distance measures (Mahalanobis)
- The algebraic structure of ReLU computation

This unification suggests that the mesa geometry is not an artifact of any single domain but reflects something fundamental about bounded prototype regions with smooth falloff—a structure that arises independently in biological signals, learned representations, and statistical models.

---

## 7. Discussion

### 7.1 What the PMG Framework Provides

The Polyhedral Mesa Gaussian offers three contributions to neural network research.

**A specific geometric object.** When asking "what does this network represent?", the PMG provides a concrete answer: look for convex polytope regions where activations are minimal, with Gaussian falloff measuring deviation from these regions. This is more specific than "features" or "representations"—it predicts a particular shape (polytope), a particular structure (plateau plus tails), and particular properties (convexity, smooth falloff). These predictions are testable.

**A geometric vocabulary.** The framework introduces precise terms for discussing neural network geometry: *plateau* (the prototype region), *mesa distance* (aggregate constraint violation), *constraint satisfaction* (zero activation). This vocabulary enables questions that are difficult to formulate otherwise: What is the volume of a concept's plateau? How many faces define it? How far is this input from the nearest plateau boundary? Such questions transform interpretability from qualitative description to quantitative measurement.

**A bridge between domains.** The PMG connects neural network computation to probability theory (Gaussian distributions, Mahalanobis distance), convex geometry (polytopes, half-space intersections), optimization (convex energy functions, hinge loss), and signal processing (Gaussian Mesa Functions). These connections are not merely analogical—they are algebraic identities. A ReLU layer *is* a constraint violation computation; the PMG *is* the probability interpretation of that computation. This bridge allows tools and insights from established fields to transfer to neural network analysis.

### 7.2 Limitations

**Negative weights.** The PMG interpretation is cleanest when aggregation weights are non-negative, implementing constraint intersection. Negative weights introduce set subtraction and relative comparisons, creating non-convex regions that the basic PMG cannot represent. A complete geometric account of negative weights—likely involving difference-of-convex (DC) decomposition [Tao & An, 1997]—remains open. Trained networks use negative weights extensively, so this gap limits the framework's direct applicability.

**Normalization.** The PMG normalization constant $A$ is intractable for complex polytopes, requiring integration over the polytope interior and exterior regions. This prevents exact probabilistic inference (computing posterior probabilities, sampling). However, for discrimination and ranking—determining whether a point is inside or outside, or which of several polytopes it is nearest to—normalization is unnecessary. The unnormalized energy $D_{PM}^2$ suffices. This aligns with energy-based model methodology [LeCun et al., 2006].

**Empirical validation.** This paper establishes the PMG as a mathematical framework; it does not empirically verify that trained networks learn PMG-structured representations. Prior work supports the constituent claims: networks use distance metrics [Oursland, 2024b], networks learn polytope partitions [Park et al., 2024; Fan et al., 2023], mesa shapes arise in biological signal modeling [Dubois et al., 2007]. But direct measurement of PMG structure in trained networks—identifying plateau regions, measuring falloff rates, comparing to predictions—remains future work.

**Single-component limitation.** A single PMG has a convex plateau. Representing non-convex concept regions requires mixtures of PMGs or hierarchical composition through network depth. The framework describes what one "mesa" looks like; how multiple mesas combine to represent complex categories is not fully specified.

### 7.3 Future Directions

The PMG framework suggests several research directions. Empirically: measure plateau geometry in trained networks, test whether activation distributions match PMG predictions, compare network robustness to plateau-boundary versus plateau-interior perturbations. Theoretically: extend the framework to handle negative weights, characterize how depth enables non-convex regions through PMG composition, establish connections to tropical geometry and max-affine splines [Balestriero & Baraniuk, 2018]. Practically: explore whether architectures explicitly designed around PMG structure (following the OffsetL2 approach in [Oursland, 2025]) offer advantages in interpretability or robustness.

The PMG does not claim to be a complete theory of neural network computation. It claims to be a useful lens—one that makes specific predictions, connects to established mathematics, and provides vocabulary for precise questions. Its value will be determined by whether that lens reveals structure that other frameworks miss.

---

## 8. Conclusion

This paper introduced the Polyhedral Mesa Gaussian as an answer to the question: what mathematical structure do ReLU networks define over their input space?

The PMG is characterized by a convex polytope plateau—a region of maximal, constant probability—surrounded by Gaussian falloff in aggregate constraint violation. The construction proceeds through a hierarchy: the univariate Mesa Mahalanobis distance introduces a plateau interval via ReLU decomposition of absolute value; the Multivariate Mesa Gaussian extends this to hyperrectangle plateaus aligned with principal axes; the full PMG relaxes orthogonality to allow arbitrary convex polytopes defined by half-space intersections.

The connection to ReLU networks is direct. A single ReLU neuron computes one-sided distance from a hyperplane—exactly a PMG component distance. A layer of neurons computes a vector of constraint violations. The region where all neurons output zero is the plateau: the prototype region that the network has learned. Activation magnitude measures deviation from this region, not feature strength.

This framework did not emerge in isolation. The mesa shape was introduced two decades ago for ECG signal processing [Dubois et al., 2007], where flat-topped waveforms required modeling that peaked Gaussians could not provide. The Mahalanobis distance has been foundational in multivariate statistics since 1936 [Mahalanobis, 1936]. Polytope structure in trained neural networks is empirically confirmed [Park et al., 2024; Fan et al., 2023]. The PMG unifies these threads, revealing them as aspects of a single geometric structure.

The interpretive implication is a shift from points to regions. Traditional neural network interpretation seeks prototype points—inputs that maximally activate neurons. The PMG framework seeks prototype regions—sets of inputs that minimally activate neurons, lying within the plateau where all constraints are satisfied. Zero activation indicates membership, not absence. This reframing aligns with empirical evidence that networks are sensitive to boundary perturbations while robust to magnitude changes [Oursland, 2024b; Oursland, 2025].

The PMG is not a new architecture or training method. It is a lens—a mathematical vocabulary for asking precise questions about neural network geometry. What is the shape of a learned concept? How many constraints define it? How far is an input from the prototype region? These questions become concrete within the PMG framework. Whether the lens proves useful is an empirical matter, to be determined by whether it reveals structure that other frameworks miss.

The plateau is where the network says "yes." Everything else is measured by how far it falls from that region.

---

## References

Arora, R., Basu, A., Mianjy, P., & Mukherjee, A. (2018). Understanding deep neural networks with rectified linear units. *International Conference on Learning Representations (ICLR)*.
Badilini, F., Vaglio, M., Dubois, R., Roussel, P., Sarapa, N., Denjoy, I., Extramiana, F., & Maison-Blanche, P. (2008). Automatic analysis of cardiac repolarization morphology using Gaussian mesa function modeling. *Journal of Electrocardiology*, 41(6), 588–594.
Balestriero, R., & Baraniuk, R. G. (2018). Mad Max: Affine spline insights into deep learning. *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 80, 473–481.
Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
Broomhead, D. S., & Lowe, D. (1988). Radial basis functions, multi-variable functional interpolation and adaptive networks. *Royal Signals and Radar Establishment Memorandum*, No. 4148.
Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning*, 20(3), 273–297.
De Maesschalck, R., Jouan-Rimbaud, D., & Massart, D. L. (2000). The Mahalanobis distance. *Chemometrics and Intelligent Laboratory Systems*, 50(1), 1–18.
Dubois, R., Maison-Blanche, P., Quenet, B., & Dreyfus, G. (2007). Automatic ECG wave extraction in long-term recordings using Gaussian mesa function models and nonlinear probability estimators. *Computer Methods and Programs in Biomedicine*, 88(3), 217–233.
Extramiana, F., Dubois, R., Vaglio, M., Roussel, P., Dreyfus, G., Badilini, F., Leenhardt, A., & Maison-Blanche, P. (2010). T-wave morphology parameters based on principal component analysis reproducibility and dependence on T-offset position. *Annals of Noninvasive Electrocardiology*, 15(1), 26–35.
Fan, Z., Wang, H., & Xu, J. (2023). Deep ReLU networks have surprisingly simple polytopes. *arXiv preprint arXiv:2305.09145*.
Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectifier neural networks. *Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)*, 315–323.
Hartman, P. (1959). On functions representable as a difference of convex functions. Pacific Journal of Mathematics, 9(3), 707–713.
Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. *Proceedings of the 32nd International Conference on Machine Learning (ICML)*, 448–456.
Jolliffe, I. T. (2002). *Principal Component Analysis* (2nd ed.). Springer.
Jolliffe, I. T. (2002). *Principal Component Analysis* (2nd ed.). Springer.
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. J. (2006). A tutorial on energy-based learning. In *Predicting Structured Data*. MIT Press.
Lee, S., Mammadov, E., & Ye, J. (2024). Defining neural network architecture through polytope structures of datasets. *Proceedings of the 41st International Conference on Machine Learning (ICML)*.
Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., & Wattenberg, M. (2025). The geometry of concepts: Sparse autoencoder feature structure. *Entropy*, 27(1), 42.
Mahalanobis, P. C. (1936). On the generalised distance in statistics. *Proceedings of the National Institute of Sciences of India*, 2(1), 49–55.
McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. *The Bulletin of Mathematical Biophysics*, 5(4), 115–133.
McLachlan, G. J. (2019). Mahalanobis distance. *Wiley Interdisciplinary Reviews: Computational Statistics*, 11(4), e1462.
Montúfar, G., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep neural networks. *Advances in Neural Information Processing Systems (NeurIPS)*, 27.
Nair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. *Proceedings of the 27th International Conference on Machine Learning (ICML)*, 807–814.
Oursland, A. (2024a). Interpreting neural networks through Mahalanobis distance. *arXiv preprint arXiv:2410.19352*.
Oursland, A. (2024b). Neural networks use distance metrics. *arXiv preprint arXiv:2411.17932*.
Oursland, A. (2025). Neural networks learn distance metrics. *arXiv preprint arXiv:2502.02103*.
Park, K., Choe, Y. J., & Veitch, V. (2024). The geometry of categorical and hierarchical concepts in large language models. *arXiv preprint arXiv:2406.01506*.
Reynolds, D. A. (2009). Gaussian mixture models. In *Encyclopedia of Biometrics* (pp. 659–663). Springer.
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. *Psychological Review*, 65(6), 386–408.
Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., Müller, E., & Kloft, M. (2018). Deep one-class classification. *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 4393–4402.
Snell, J., Swersky, K., & Zemel, R. (2017). Prototypical networks for few-shot learning. *Advances in Neural Information Processing Systems (NeurIPS)*, 30.
Tao, P. D., & An, L. H. (1997). Convex analysis approach to D.C. programming: Theory, algorithms and applications. *Acta Mathematica Vietnamica*, 22(1), 289–355.
Vershynin, R. (2018). High-Dimensional Probability. Cambridge University Press.
Weinberger, K. Q., & Saul, L. K. (2009). Distance metric learning for large margin nearest neighbor classification. *Journal of Machine Learning Research*, 10, 207–244.
Ziegler, G. M. (1995). *Lectures on Polytopes*. Springer.
